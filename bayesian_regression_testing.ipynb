{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### General information referring to some of the code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**patsy**\n",
    "*Generating design matrices*\n",
    "We include the intercept once in the model via the intercept parameter, to prevent having additional '1' columns in basis functions include_intercept is set to False and automatic creation of '1' column of patsy is prevented by '-1' in the formula.\n",
    "\n",
    "*Passing design matrices to PyMc/ Stan*\n",
    "For the spline regression we generate our design matrices using patsy. This returns an object of the class 'DesignMatrix' which contains additional metadata comparede to a numpy array. However in comparioson to numpy arrays, those objects of the DesignMatrix clss do not support pickling.\n",
    "Therefore we convert the matrices to numpy arrays by np.asarray() - essentially dropping the metadata - before passing them to the model.\n",
    "THe additional argument order=\"F\" is used to have the array in Fortran order which is preferred by PyMC3 and Stan.\n",
    "\n",
    "**Understanding posterior predictives and cross validation**\n",
    "(1) We specify a model and incorporate some prior belief for the model parameters.\n",
    "(2) Using the training data and the MCMC algorithm we update our prior beliefs to better represent the data we obeserve. Thereby we obtain posterior distributions for our model parameters.\n",
    "(3) With new data (our evaluation set), we can verify our model. Notice, that the posterior distributions of all parameters we defined in our model remain unchanged. In our specific example only mu is individual for each parameter as it is dependent on the covariates of a observation. This explains how we obtain a different mu for every obeservation if the covariates aren't identical. To verify the model we draw samples from the posterior distribution of each observation and calculate the CRPS.\n",
    "(how are the posterior predictive samples obtained: we draw a set of paramters based on their underlying posterior distribution and use those to draw one sample. This is repeated several times to reflect the uncertainty within  the posteriors for this obeservation.\n",
    "\n",
    "**Implementing cross validation**\n",
    "PyStan: Add corresponding code to the 'generate quantities block' to generate the posterior predictive samples based on obtained posteriors and new data\n",
    "PyMc: Define covariates as \"theano.shared\" variables before feeding them into the model and integrate evaluation data via the \"set_value()\" function. Afterwards, pm.sample_posterior_predictive() will generate predictives for the evaluation data only. \n",
    "\n",
    "**Transforming data**\n",
    "According to Bretzger, Lang (2004) it is desirable to standardize out target variable as \"the amount of smoothness allowed by a particular prior specification depends (weakly) on the scale of the responses\". However, initially in Stan this rose an error originating from working with a negative binomial distribution. Further inspection is necessary.\n",
    "\n",
    "**Defining knots for splines**\n",
    "So far we use equally-spaced knots for our spline regression. As our data is zero-inflated and overdispersed this leads to having the majority of the data within a minorty of the knots. Quantile-based knots are therefore a valid option. However, the P-Splines definition through a penalty matrix is not compatible with quantile-based knots. For an integration of quantile-based knots further investigation is necessary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## I. Prerequisites"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Import Packages**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from data_gathering import gather_data_actuals, gather_data_features\n",
    "from data_preparation import preprocess_data, generate_knots, generate_spline_design_matrix\n",
    "from data_exploration import calculate_data_characteristics\n",
    "from data_modelling_bayes import calculate_crps, generate_penalty_matrix, generate_all_dictionaries, ml_flow_tracking, count_divergences\n",
    "from help_functions import MLFlowServer\n",
    "from importlib import reload\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:24:06.186166200Z",
     "start_time": "2023-09-06T16:24:01.956399400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Reload modules to reflect changes in imported files\n",
    "import data_preparation\n",
    "reload(data_preparation)\n",
    "from data_preparation import generate_spline_design_matrix\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:24:06.200037600Z",
     "start_time": "2023-09-06T16:24:06.187161500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "pymc = False\n",
    "stan = True\n",
    "example_export = False\n",
    "start_server = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:24:06.332574600Z",
     "start_time": "2023-09-06T16:24:06.202000400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**PyMc specific packages**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "if pymc:\n",
    "    import pymc as pm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:24:06.332574600Z",
     "start_time": "2023-09-06T16:24:06.217962200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**PyStan specific packages**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "if stan:\n",
    "    import pystan"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:24:07.192809600Z",
     "start_time": "2023-09-06T16:24:06.234912600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=4, connect=4, read=5, redirect=5, status=5)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002619F71D648>: Failed to establish a new connection: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte')': /api/2.0/mlflow/experiments/get-by-name?experiment_name=bayesian_models\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\urllib3\\util\\connection.py\u001B[0m in \u001B[0;36mcreate_connection\u001B[1;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[0;32m     72\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mres\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msocket\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetaddrinfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhost\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mport\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfamily\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msocket\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSOCK_STREAM\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 73\u001B[1;33m         \u001B[0maf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msocktype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mproto\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcanonname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msa\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     74\u001B[0m         \u001B[0msock\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mConnectionRefusedError\u001B[0m: [WinError 10061] Es konnte keine Verbindung hergestellt werden, da der Zielcomputer die Verbindung verweigerte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_6436\\4088893918.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[1;31m# Start mlflow server: Necessary before every first mlflow_logging\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[0mserver\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMLFlowServer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mexperiment_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"bayesian_models\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m     \u001B[0mserver\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\PycharmProjects\\bachelor_thesis\\help_functions.py\u001B[0m in \u001B[0;36mstart\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     61\u001B[0m         \u001B[0mtime\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msleep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m5\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m         \u001B[0mmlflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_tracking_uri\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtracking_uri\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 63\u001B[1;33m         \u001B[0mmlflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mset_experiment\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperiment_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     64\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     65\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mstop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\mlflow\\tracking\\fluent.py\u001B[0m in \u001B[0;36mset_experiment\u001B[1;34m(experiment_name, experiment_id)\u001B[0m\n\u001B[0;32m    111\u001B[0m     \u001B[0mclient\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mMlflowClient\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    112\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mexperiment_id\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 113\u001B[1;33m         \u001B[0mexperiment\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mclient\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_experiment_by_name\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mexperiment_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    114\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mexperiment\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    115\u001B[0m             _logger.info(\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\mlflow\\tracking\\client.py\u001B[0m in \u001B[0;36mget_experiment_by_name\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    583\u001B[0m             \u001B[0mLifecycle_stage\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mactive\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    584\u001B[0m         \"\"\"\n\u001B[1;32m--> 585\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_tracking_client\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_experiment_by_name\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    586\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    587\u001B[0m     def create_experiment(\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py\u001B[0m in \u001B[0;36mget_experiment_by_name\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m    239\u001B[0m         \u001B[1;33m:\u001B[0m\u001B[1;32mreturn\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;33m:\u001B[0m\u001B[0mpy\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;32mclass\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mmlflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mentities\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mExperiment\u001B[0m\u001B[0;31m`\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    240\u001B[0m         \"\"\"\n\u001B[1;32m--> 241\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstore\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_experiment_by_name\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    242\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    243\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mcreate_experiment\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0martifact_location\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtags\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py\u001B[0m in \u001B[0;36mget_experiment_by_name\u001B[1;34m(self, experiment_name)\u001B[0m\n\u001B[0;32m    317\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    318\u001B[0m             \u001B[0mreq_body\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmessage_to_json\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mGetExperimentByName\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mexperiment_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mexperiment_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 319\u001B[1;33m             \u001B[0mresponse_proto\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_call_endpoint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mGetExperimentByName\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreq_body\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    320\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mExperiment\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_proto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresponse_proto\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexperiment\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    321\u001B[0m         \u001B[1;32mexcept\u001B[0m \u001B[0mMlflowException\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py\u001B[0m in \u001B[0;36m_call_endpoint\u001B[1;34m(self, api, json_body)\u001B[0m\n\u001B[0;32m     55\u001B[0m         \u001B[0mendpoint\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_METHOD_TO_INFO\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mapi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     56\u001B[0m         \u001B[0mresponse_proto\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mapi\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mResponse\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 57\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mcall_endpoint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_host_creds\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mendpoint\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mjson_body\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mresponse_proto\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     58\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     59\u001B[0m     def list_experiments(\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\mlflow\\utils\\rest_utils.py\u001B[0m in \u001B[0;36mcall_endpoint\u001B[1;34m(host_creds, endpoint, method, json_body, response_proto)\u001B[0m\n\u001B[0;32m    272\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mmethod\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m\"GET\"\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    273\u001B[0m         response = http_request(\n\u001B[1;32m--> 274\u001B[1;33m             \u001B[0mhost_creds\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mhost_creds\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mendpoint\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mendpoint\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mjson_body\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    275\u001B[0m         )\n\u001B[0;32m    276\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\mlflow\\utils\\rest_utils.py\u001B[0m in \u001B[0;36mhttp_request\u001B[1;34m(host_creds, endpoint, method, max_retries, backoff_factor, retry_codes, timeout, **kwargs)\u001B[0m\n\u001B[0;32m    173\u001B[0m             \u001B[0mverify\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mverify\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    174\u001B[0m             \u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 175\u001B[1;33m             \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    176\u001B[0m         )\n\u001B[0;32m    177\u001B[0m     \u001B[1;32mexcept\u001B[0m \u001B[0mrequests\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mexceptions\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mTimeout\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mto\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\mlflow\\utils\\rest_utils.py\u001B[0m in \u001B[0;36m_get_http_response_with_retries\u001B[1;34m(method, url, max_retries, backoff_factor, retry_codes, **kwargs)\u001B[0m\n\u001B[0;32m     95\u001B[0m     \"\"\"\n\u001B[0;32m     96\u001B[0m     \u001B[0msession\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_get_request_session\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmax_retries\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbackoff_factor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretry_codes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 97\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0msession\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     98\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     99\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\requests\\sessions.py\u001B[0m in \u001B[0;36mrequest\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    587\u001B[0m         \u001B[0mresp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprep\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0msend_kwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    588\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 589\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mresp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    590\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    591\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\requests\\sessions.py\u001B[0m in \u001B[0;36msend\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    701\u001B[0m         \u001B[0mr\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0madapter\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mrequest\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    702\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 703\u001B[1;33m         \u001B[1;31m# Total elapsed time of the request (approximately)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    704\u001B[0m         \u001B[0melapsed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpreferred_clock\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    705\u001B[0m         \u001B[0mr\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0melapsed\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtimedelta\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mseconds\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0melapsed\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\requests\\adapters.py\u001B[0m in \u001B[0;36msend\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    495\u001B[0m                     \u001B[0massert_same_host\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    496\u001B[0m                     \u001B[0mpreload_content\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 497\u001B[1;33m                     \u001B[0mdecode_content\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    498\u001B[0m                     \u001B[0mretries\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax_retries\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    499\u001B[0m                     \u001B[0mtimeout\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\urllib3\\connectionpool.py\u001B[0m in \u001B[0;36murlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[0;32m    887\u001B[0m                 \u001B[0mchunked\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mchunked\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m                 \u001B[0mbody_pos\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mbody_pos\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m                 \u001B[1;33m**\u001B[0m\u001B[0mresponse_kw\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m             )\n\u001B[0;32m    891\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\urllib3\\connectionpool.py\u001B[0m in \u001B[0;36murlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[0;32m    800\u001B[0m                 \u001B[0mrelease_this_conn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    801\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 802\u001B[1;33m             \u001B[1;32mif\u001B[0m \u001B[0mrelease_this_conn\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    803\u001B[0m                 \u001B[1;31m# Put the connection back to be reused. If the connection is\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    804\u001B[0m                 \u001B[1;31m# expired then it will be None, which will get replaced with a\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\urllib3\\connectionpool.py\u001B[0m in \u001B[0;36m_make_request\u001B[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[0;32m    502\u001B[0m         \"\"\"\n\u001B[0;32m    503\u001B[0m         \u001B[0mCheck\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mgiven\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0murl\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m \u001B[1;32mis\u001B[0m \u001B[0ma\u001B[0m \u001B[0mmember\u001B[0m \u001B[0mof\u001B[0m \u001B[0mthe\u001B[0m \u001B[0msame\u001B[0m \u001B[0mhost\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mthis\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 504\u001B[1;33m         \u001B[0mconnection\u001B[0m \u001B[0mpool\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    505\u001B[0m         \"\"\"\n\u001B[0;32m    506\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"/\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\urllib3\\connection.py\u001B[0m in \u001B[0;36mrequest\u001B[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[0;32m    393\u001B[0m         \u001B[1;31m# trusted_root_certs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    394\u001B[0m         \u001B[0mdefault_ssl_context\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 395\u001B[1;33m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mssl_context\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    396\u001B[0m             \u001B[0mdefault_ssl_context\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    397\u001B[0m             self.ssl_context = create_urllib3_context(\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\http\\client.py\u001B[0m in \u001B[0;36mendheaders\u001B[1;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[0;32m   1274\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1275\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mCannotSendHeader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1276\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_send_output\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmessage_body\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencode_chunked\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mencode_chunked\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1277\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1278\u001B[0m     def request(self, method, url, body=None, headers={}, *,\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\http\\client.py\u001B[0m in \u001B[0;36m_send_output\u001B[1;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[0;32m   1034\u001B[0m         \u001B[0mmsg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34mb\"\\r\\n\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_buffer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1035\u001B[0m         \u001B[1;32mdel\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_buffer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1036\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmsg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1037\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1038\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mmessage_body\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\http\\client.py\u001B[0m in \u001B[0;36msend\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m    974\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msock\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    975\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_open\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 976\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconnect\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    977\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    978\u001B[0m                 \u001B[1;32mraise\u001B[0m \u001B[0mNotConnected\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\urllib3\\connection.py\u001B[0m in \u001B[0;36mconnect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    241\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mrequest_chunked\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbody\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheaders\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    242\u001B[0m         \"\"\"\n\u001B[1;32m--> 243\u001B[1;33m         \u001B[0mAlternative\u001B[0m \u001B[0mto\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mcommon\u001B[0m \u001B[0mrequest\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mwhich\u001B[0m \u001B[0msends\u001B[0m \u001B[0mthe\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    244\u001B[0m         \u001B[0mbody\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mchunked\u001B[0m \u001B[0mencoding\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mone\u001B[0m \u001B[0mblock\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    245\u001B[0m         \"\"\"\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\urllib3\\connection.py\u001B[0m in \u001B[0;36m_new_conn\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    205\u001B[0m         \u001B[0mconn\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_new_conn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    206\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_prepare_conn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 207\u001B[1;33m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    208\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mputrequest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmethod\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0murl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    209\u001B[0m         \u001B[1;34m\"\"\" \"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\urllib3\\util\\connection.py\u001B[0m in \u001B[0;36mcreate_connection\u001B[1;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[0;32m     71\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     72\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mres\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msocket\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgetaddrinfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhost\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mport\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfamily\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msocket\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mSOCK_STREAM\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 73\u001B[1;33m         \u001B[0maf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msocktype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mproto\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcanonname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msa\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mres\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     74\u001B[0m         \u001B[0msock\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     75\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if start_server:\n",
    "    # Start mlflow server: Necessary before every first mlflow_logging\n",
    "    server = MLFlowServer(experiment_name=\"bayesian_models\")\n",
    "    server.start()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:43:01.445249900Z",
     "start_time": "2023-09-06T16:42:52.272778400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "server.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:37:06.883991Z",
     "start_time": "2023-09-06T16:37:06.745189200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Define functions**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Set style and generate random seed**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "az.style.use(\"arviz-darkgrid\")\n",
    "RANDOM_SEED = 58"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:06.642545400Z",
     "start_time": "2023-09-06T16:27:05.556677Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## II. Data Preparation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "data_cm_actual_2018, data_cm_actual_2019, data_cm_actual_2020, data_cm_actual_2021, data_cm_actual_allyears \\\n",
    "    = gather_data_actuals()\n",
    "# Load features data\n",
    "data_cm_features_2017, data_cm_features_2018, data_cm_features_2019, data_cm_features_2020, data_cm_features_allyears \\\n",
    "    = gather_data_features()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:06.946334900Z",
     "start_time": "2023-09-06T16:27:05.573641200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Define model specifications corresponding to mlflow log"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Data used in model**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uwe Drauz\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\pandas\\core\\arraylike.py:358: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "predictors = ['ged_sb_tlag_1', 'ged_sb_tsum_24']\n",
    "target_variable = ['ged_sb']\n",
    "country_name = 'Syria'\n",
    "df_Y_non_std_X_std, df_Y_std_X_std, df_Y_non_std_X_std_log, df_Y_std_X_std_log = (preprocess_data(data_cm_features_allyears, predictors, target_variable, country_name))\n",
    "df = df_Y_non_std_X_std"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:06.947332100Z",
     "start_time": "2023-09-06T16:27:06.442388100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Note: Prelimanary definition of evaluation data till better integration of cross validation\n",
    "# Take last 24 obeservations of training data a\n",
    "df_eval = df.iloc[-24:]\n",
    "# Select 3rd till 14th observation (equals year 2019 here) as evaluation data\n",
    "df_eval = df_eval.iloc[3:14]\n",
    "# Cut the last 24 observations from the training data\n",
    "df = df.iloc[:-24]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:06.948329700Z",
     "start_time": "2023-09-06T16:27:06.522400100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "dataX1 = df.ged_sb_tlag_1.to_numpy()\n",
    "dataX2 = df.ged_sb_tsum_24.to_numpy()\n",
    "dataY = df.ged_sb.to_numpy()\n",
    "dataY_df = pd.DataFrame(df[\"ged_sb\"])\n",
    "data_characteristics_train = calculate_data_characteristics(df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:06.989362600Z",
     "start_time": "2023-09-06T16:27:06.531376400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "dataX1_eval = df_eval.ged_sb_tlag_1.to_numpy()\n",
    "dataX2_eval = df_eval.ged_sb_tsum_24.to_numpy()\n",
    "dataY_eval = df_eval.ged_sb.to_numpy()\n",
    "dataY_eval_df = pd.DataFrame(df_eval[\"ged_sb\"])\n",
    "data_characteristics_eval = calculate_data_characteristics(df_eval)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:06.989623800Z",
     "start_time": "2023-09-06T16:27:06.580571600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "data_transformation = \"target variable not standardized, input data standardized\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:06.989623800Z",
     "start_time": "2023-09-06T16:27:06.609659700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export to test in R\n",
    "if example_export:\n",
    "    df.to_parquet(fr'C:\\Users\\Uwe Drauz\\Documents\\bachelor_thesis_local\\personal_competition_data\\fatality_data_{country_name}.parquet')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-06T16:25:24.631425100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Model description"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "model_name_gauss = 'bayesian_regression'\n",
    "model_name_nb_half_normal = 'bayesian_regression'\n",
    "model_name_nb_gamma = 'bayesian_regression'\n",
    "model_name_splines = 'bayesian_spline_regression'\n",
    "model_name_penalized_splines_grw = 'bayesian_penalized_spline_regression_gaussian_random_walk'\n",
    "model_name_penalized_splines_penalty = 'bayesian_penalized_spline_regression_penalty_potential'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:07.039343800Z",
     "start_time": "2023-09-06T16:27:06.625673400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "prior_specifications_gauss = {\n",
    "    \"intercept\": \"Normal\",\n",
    "    \"slopesX1\": \"Normal\", \n",
    "    \"slopesX2\": \"Normal\", \n",
    "    \"likelihood\": \"Gaussian\",\n",
    "    \"sigma\": \"Normal\", \n",
    "}\n",
    "prior_specifications_nb_half_normal = {\n",
    "    \"intercept\": \"Normal\",\n",
    "    \"slopesX1\": \"Normal\", \n",
    "    \"slopesX2\": \"Normal\", \n",
    "    \"likelihood\": \"NegativeBinomial\",\n",
    "    \"alpha\": \"HalfNormal\", \n",
    "}\n",
    "prior_specifications_nb_gamma = {\n",
    "    \"intercept\": \"Normal\",\n",
    "    \"slopesX1\": \"Normal\", \n",
    "    \"slopesX2\": \"Normal\", \n",
    "    \"likelihood\": \"NegativeBinomial\",\n",
    "    \"alpha\": \"Gamma\", \n",
    "}\n",
    "prior_specifications_splines = {\n",
    "    \"intercept\": \"Normal\",\n",
    "    \"slopesX1\": \"Normal\", \n",
    "    \"slopesX2\": \"Normal\", \n",
    "    \"likelihood\": \"NegativeBinomial\",\n",
    "    \"alpha\": \"Gamma\", \n",
    "}\n",
    "prior_specifications_penalized_splines_grw = {\n",
    "    \"intercept\": \"Normal\",\n",
    "    \"inital_distribution\": \"Normal\",\n",
    "    \"slopesX1\": \"GaussianRandomWalk\", \n",
    "    \"tauX1\": \"InverseGamma\",\n",
    "    \"slopesX2\": \"GaussianRandomWalk\", \n",
    "    \"tauX2\": \"InverseGamma\",\n",
    "    \"likelihood\": \"NegativeBinomial\",\n",
    "    \"alpha\": \"Gamma\", \n",
    "}\n",
    "prior_specifications_penalized_splines_penalty = {\n",
    "    \"intercept\": \"Normal\",\n",
    "    \"slopesX1_0\": \"Normal\",\n",
    "    \"slopesX1_rest\": \"Normal\", \n",
    "    \"tauX1\": \"InverseGamma\",\n",
    "    \"slopesX2_0\": \"Normal\",\n",
    "    \"slopesX2_rest\": \"Normal\", \n",
    "    \"tauX2\": \"InverseGamma\",\n",
    "    \"likelihood\": \"NegativeBinomial\",\n",
    "    \"alpha\": \"Gamma\", \n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:07.040338200Z",
     "start_time": "2023-09-06T16:27:06.662716800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Model hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# PARAMETERS RELATED TO SPLINES\n",
    "num_knots_X1 = 10\n",
    "num_knots_X2 = 10\n",
    "spline_degree = 3\n",
    "no_spline_coefficients_X1 = num_knots_X1 + spline_degree\n",
    "no_spline_coefficients_X2 = num_knots_X2 + spline_degree"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:07.040338200Z",
     "start_time": "2023-09-06T16:27:06.675233800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# name scheme: y ~ dist1(z), x ~ dist2(y)\n",
    "# opt(component name) + dist1 + z + dist2 + y\n",
    "# PARAMETERS RELATED TO PRIORS\n",
    "# Define variables for tau priors\n",
    "tauX1_ig_alpha_gaussian_sigma = tauX2_ig_alpha_gaussian_sigma = 1\n",
    "tauX1_ig_beta_gaussian_sigma = tauX2_ig_beta_gaussian_sigma = 0.005\n",
    "\n",
    "# Define variables for alpha prior of gamma distribution of alpha of negative binomial distribution\n",
    "gamma_alpha_nb_alpha = 0.1\n",
    "gamma_beta_nb_alpha = 0.1\n",
    "\n",
    "# Define variables for prior distribution of intercept\n",
    "intercept_gaussian_mu = 0\n",
    "intercept_gaussian_sigma = 5\n",
    "\n",
    "# Define standard deviation for Half Normal distribution of  alpha parameter of likelihood negative binomial distribution\n",
    "hgaussian_sigma_nb_alpha = 2.5\n",
    "\n",
    "# Define standard deviation for Normal distribution of sigma of likelihood gaussian distribution\n",
    "gaussian_sigma_gaussian_sigma = 2.5\n",
    "\n",
    "# Define parameters for gaussian distribution of regression coefficients\n",
    "beta_gaussian_mu = 0.0\n",
    "beta_gaussian_sigma = 1.0\n",
    "\n",
    "# Define paramters for half gaussian distribution of intercepts\n",
    "intercept_hgaussian_mu = 0\n",
    "intercept_hgaussian_sigma = 1\n",
    "\n",
    "# Define parameters for beta diffuse prior\n",
    "diffuse_prior_beta_mu = 0\n",
    "diffuse_prior_beta_sigma = 100\n",
    "\n",
    "# Define sigma of half gaussian distribution of sigma for gaussian likelihood\n",
    "hgaussian_sigma_gaussian_sigma = 2.5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:07.041333300Z",
     "start_time": "2023-09-06T16:27:06.690193500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Sampling hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Define number of tuning iterations\n",
    "tune_default = 2000\n",
    "# Define number of sampling iterations\n",
    "draws_default = 1000\n",
    "# Define target acceptance rate\n",
    "target_accept_default = 0.95\n",
    "# Define number of chains\n",
    "no_chains_default = 4\n",
    "# Define number of posterior predictive samples\n",
    "no_ppc_samples_default = 500"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:07.041333300Z",
     "start_time": "2023-09-06T16:27:06.703158900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Define dictionary for implicit handling of data in PyMc\n",
    "COORDS_gauss = {\"predictors\": ['ged_sb_tlag_1', 'ged_sb_tsum_24'], \"obs_idx\": df.index}\n",
    "COORDS_nb_half_normal = {\"predictors\": ['ged_sb_tlag_1', 'ged_sb_tsum_24'], \"obs_idx\": df.index}\n",
    "COORDS_nb_gamma = {\"predictors\": ['ged_sb_tlag_1', 'ged_sb_tsum_24'], \"obs_idx\": df.index}\n",
    "COORDS_splines = {\"obs_idx\": df.index, \"splines_x1\": np.arange(no_spline_coefficients_X1), \"splines_x2\": np.arange(no_spline_coefficients_X2)}\n",
    "COORDS_penalized_splines_rw = {\"obs_idx\": df.index, \"splines_x1\": no_spline_coefficients_X1, \"splines_x2\": no_spline_coefficients_X2}\n",
    "COORDS_penalized_splines_penalty = \\\n",
    "    {\"obs_idx\": df.index, \"splines_x1_rest\": np.arange(no_spline_coefficients_X1 -1), \"splines_x2_rest\": np.arange(no_spline_coefficients_X2 -1)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:07.042330700Z",
     "start_time": "2023-09-06T16:27:06.735305700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. Calculate design matrices for implementation of splines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# We define the knot list such that we'll have num_knots knot inbetween the boundaries of the data\n",
    "knot_list_X1 = generate_knots(dataX1, num_knots_X1)\n",
    "knot_list_X2 = generate_knots(dataX2, num_knots_X2)\n",
    "\n",
    "# Define the basis functions\n",
    "basis_X1 = generate_spline_design_matrix(dataX1, knot_list_X1, spline_degree)\n",
    "basis_X2 = generate_spline_design_matrix(dataX2, knot_list_X2, spline_degree)\n",
    "\n",
    "# Define the penalty matrices\n",
    "K_X1 = generate_penalty_matrix(basis_X1.shape[1])\n",
    "K_X2 = generate_penalty_matrix(basis_X2.shape[1])\n",
    "\n",
    "if basis_X1.shape[1] != no_spline_coefficients_X1:\n",
    "    print(\"Number of coefficients as per basis function of regressor 1 did not match predefined number of coefficients in spline. Changed it by considering dimensions of basis design matrix\")\n",
    "    no_spline_coefficients_X1 = basis_X1.shape[1]\n",
    "if basis_X2.shape[1] != no_spline_coefficients_X2:\n",
    "    print(\"Number of coefficients as per basis function of regressor 2 did not match predefined number of coefficients in spline. Changed it by considering dimensions of basis design matrix\")\n",
    "    no_spline_coefficients_X2 = basis_X2.shape[1]\n",
    "\n",
    "# Define the design matrices for the evaluation data\n",
    "knot_list_X1_eval = generate_knots(dataX1_eval, num_knots_X1)\n",
    "knot_list_X2_eval = generate_knots(dataX2_eval, num_knots_X2)\n",
    "\n",
    "basis_X1_eval = generate_spline_design_matrix(dataX1_eval, knot_list_X1_eval, spline_degree)\n",
    "basis_X2_eval = generate_spline_design_matrix(dataX2_eval, knot_list_X2_eval, spline_degree)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:07.044327400Z",
     "start_time": "2023-09-06T16:27:06.752260500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## III.1 PyMc Model Building"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Build Bayesian model with Gaussian distribution"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start= time.time()\n",
    "with pm.Model(coords=COORDS_gauss) as bayesian_model_gauss:\n",
    "    # Define the priors for regressors and standard deviation\n",
    "    intercept = pm.Normal('intercept', intercept_gaussian_mu, intercept_gaussian_sigma)\n",
    "    beta = pm.Normal(\"slopes\", beta_gaussian_mu, beta_gaussian_sigma, dims=\"predictors\")\n",
    "    sigma = pm.HalfNormal(\"sigma\", hgaussian_sigma_gaussian_sigma)\n",
    "\n",
    "    # Specify the data\n",
    "    X1 = pm.ConstantData(\"fatality lag t=1\", dataX1, dims=\"obs_idx\")\n",
    "    X2 = pm.ConstantData(\"fatality rolling average t=24\", dataX2, dims=\"obs_idx\")\n",
    "    Y = pm.ConstantData(\"fatility count\", dataY, dims=\"obs_idx\")\n",
    "\n",
    "    # Specify mean of gaussian distribution\n",
    "    mu = intercept +  beta[0] * X1 + beta[1] * X2\n",
    "    # mean for target variable\n",
    "    obs = pm.Normal(\"obs\", mu=mu, sigma=sigma, observed=Y, dims=\"obs_idx\")\n",
    "\n",
    "    # Run the sampling using the No-U-Turn Sampler (NUTS) for the specified number of samples\n",
    "    idata_bayesian_gauss = pm.sample(draws=draws_default, tune=tune_default, random_seed=RANDOM_SEED, target_accept=target_accept_default)\n",
    "\n",
    "sampling_time_gauss = time.time() - start"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with bayesian_model_gauss:\n",
    "    ppc_prior_gauss = pm.sample_prior_predictive(no_ppc_samples_default, random_seed=RANDOM_SEED)\n",
    "    ppc_posterior_gauss = pm.sample_posterior_predictive(idata_bayesian_gauss, extend_inferencedata=True, random_seed=RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crps_gauss, crps_average_gauss = calculate_crps(idata_bayesian_gauss, actuals=dataY_df, posterior_predictive='obs');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dictionary of parameters used in the model:\n",
    "divergences_gauss = count_divergences(idata_bayesian_gauss)\n",
    "all_dictionaries_gauss = generate_all_dictionaries(\n",
    "    training_data=df, \n",
    "    evaluation_data=df_eval,\n",
    "    data_characteristics_train=data_characteristics_train, \n",
    "    data_characteristics_eval=data_characteristics_eval,\n",
    "    data_transformation=data_transformation, \n",
    "    country_name=country_name, \n",
    "    predictors=predictors, \n",
    "    model_name=model_name_gauss, \n",
    "    prior_specifications=prior_specifications_gauss,\n",
    "    intercept_hyperparameters= (intercept_gaussian_mu, intercept_gaussian_sigma),\n",
    "    beta_hyperparameters=(beta_gaussian_mu, beta_gaussian_sigma),\n",
    "    sigma_gaussian_hyperparameters=hgaussian_sigma_gaussian_sigma,\n",
    "    tuning_iterations=tune_default,\n",
    "    sampling_iterations=draws_default,\n",
    "    target_acceptance_rate=target_accept_default,\n",
    "    chains=no_chains_default,\n",
    "    divergences=divergences_gauss,\n",
    "    sampling_time=sampling_time_gauss, \n",
    "    idata=idata_bayesian_gauss,\n",
    "    crps_score_train=crps_average_gauss,\n",
    "    crps_score_eval=crps_average_gauss)\n",
    "ml_flow_tracking(**all_dictionaries_gauss)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Build Bayesian model with Negative Binomial distribution (alpha HalfNormal distributed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build the model\n",
    "start = time.time()\n",
    "with pm.Model(coords=COORDS_nb_half_normal) as bayesian_model_nb_half_normal:\n",
    "    # Define the priors for regressors and  negative binomial over-dispersion parameter\n",
    "    intercept = pm.Normal(\"intercept\", mu=intercept_gaussian_mu, sigma=intercept_gaussian_sigma)\n",
    "    beta = pm.Normal(\"slopes\", beta_gaussian_mu, beta_gaussian_sigma, dims=\"predictors\")\n",
    "    alpha = pm.HalfNormal('alpha', sigma=hgaussian_sigma_nb_alpha)\n",
    "\n",
    "    # Specify the data\n",
    "    X1 = pm.ConstantData(\"fatality lag t=1\", dataX1, dims=\"obs_idx\")\n",
    "    X2 = pm.ConstantData(\"fatality rolling average t=24\", dataX2, dims=\"obs_idx\")\n",
    "    Y = pm.ConstantData(\"fatility count\", dataY, dims=\"obs_idx\")\n",
    "\n",
    "    # Note: Possibly an option to change to pm.Deterministic\n",
    "    # Define mean of negative binomial distribution\n",
    "    mu = pm.math.exp(intercept + beta[0] * X1 + beta[1] * X2)\n",
    "\n",
    "    # Define the likelihood\n",
    "    obs = pm.NegativeBinomial(\"obs\", mu=mu, alpha=alpha, observed=Y, dims=\"obs_idx\")\n",
    "\n",
    "    # Note: Optionally define cores and chains\n",
    "    # Run the sampling using the No-U-Turn Sampler (NUTS) for the specified number of samples\n",
    "    idata_bayesian_nb_half_normal = pm.sample(draws=draws_default, tune=tune_default, random_seed=RANDOM_SEED, target_accept=target_accept_default)\n",
    "\n",
    "sampling_time_nb_half_normal = time.time() - start"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    with bayesian_model_nb_half_normal:\n",
    "        ppc_posterior_nb_half_normal = pm.sample_posterior_predictive(idata_bayesian_nb_half_normal, extend_inferencedata=True, random_seed=RANDOM_SEED)\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)\n",
    "try:\n",
    "    with bayesian_model_nb_half_normal:\n",
    "        ppc_prior_nb_half_normal = pm.sample_prior_predictive(no_ppc_samples_default, random_seed=RANDOM_SEED)\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crps_nb_half_normal, crps_average_nb_half_normal = calculate_crps(idata_bayesian_nb_half_normal, actuals=dataY_df, posterior_predictive='obs');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dictionary of parameters used in the model:\n",
    "divergences_nb_half_normal = count_divergences(idata_bayesian_nb_half_normal)\n",
    "all_dictionaries_nb_half_normal = generate_all_dictionaries(\n",
    "    training_data=df, \n",
    "    evaluation_data=df_eval,\n",
    "    data_characteristics_train=data_characteristics_train, \n",
    "    data_characteristics_eval=data_characteristics_eval,\n",
    "    data_transformation=data_transformation, \n",
    "    country_name=country_name, \n",
    "    predictors=predictors, \n",
    "    model_name=model_name_nb_half_normal, \n",
    "    prior_specifications=prior_specifications_nb_half_normal,\n",
    "    alpha_nb_hyperparameters=hgaussian_sigma_nb_alpha,\n",
    "    intercept_hyperparameters= (intercept_gaussian_mu, intercept_gaussian_sigma),\n",
    "    beta_hyperparameters=(beta_gaussian_mu, beta_gaussian_sigma),\n",
    "    tuning_iterations=tune_default,\n",
    "    sampling_iterations=draws_default,\n",
    "    target_acceptance_rate=target_accept_default,\n",
    "    chains=no_chains_default,\n",
    "    divergences=divergences_nb_half_normal,\n",
    "    sampling_time=sampling_time_nb_half_normal, \n",
    "    idata=idata_bayesian_nb_half_normal,\n",
    "    crps_score_train=crps_average_nb_half_normal,\n",
    "    crps_score_eval=crps_average_nb_half_normal)\n",
    "ml_flow_tracking(**all_dictionaries_nb_half_normal)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Build Bayesian model with Negative Binomial distribution (alpha Gamma distributed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build the model\n",
    "start = time.time()\n",
    "with pm.Model(coords=COORDS_nb_gamma) as bayesian_model_nb_gamma:\n",
    "    # Define the priors for regressors and  negative binomial over-dispersion parameter\n",
    "    intercept = pm.Normal(\"intercept\", mu=intercept_gaussian_mu, sigma=intercept_gaussian_sigma)\n",
    "    beta = pm.Normal(\"slopes\", beta_gaussian_mu, beta_gaussian_sigma, dims=\"predictors\")\n",
    "    alpha = pm.Gamma('alpha', gamma_alpha_nb_alpha, gamma_beta_nb_alpha)\n",
    "\n",
    "    # Specify the data\n",
    "    X1 = pm.ConstantData(\"fatality lag t=1\", dataX1, dims=\"obs_idx\")\n",
    "    X2 = pm.ConstantData(\"fatality rolling average t=24\", dataX2, dims=\"obs_idx\")\n",
    "    Y = pm.ConstantData(\"fatility count\", dataY, dims=\"obs_idx\")\n",
    "\n",
    "    # Note: Possibly an option to change to pm.Deterministic\n",
    "    # Define mean of negative binomial distribution\n",
    "    mu = pm.math.exp(intercept + beta[0] * X1 + beta[1] * X2)\n",
    "\n",
    "    # Define the likelihood\n",
    "    obs = pm.NegativeBinomial(\"obs\", mu=mu, alpha=alpha, observed=Y, dims=\"obs_idx\")\n",
    "\n",
    "    # Note: Optionally define cores and chains\n",
    "    # Run the sampling using the No-U-Turn Sampler (NUTS) for the specified number of samples\n",
    "    idata_bayesian_nb_gamma = pm.sample(draws=draws_default, tune=tune_default, random_seed=RANDOM_SEED, target_accept=target_accept_default)\n",
    "\n",
    "sampling_time_nb_gamma = time.time() - start"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    with bayesian_model_nb_gamma:\n",
    "        ppc_posterior_nb_gamma = pm.sample_posterior_predictive(idata_bayesian_nb_gamma, extend_inferencedata=True, random_seed=RANDOM_SEED)\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)\n",
    "try:\n",
    "    with bayesian_model_nb_gamma:\n",
    "        ppc_prior_nb_gamma = pm.sample_prior_predictive(no_ppc_samples_default, random_seed=RANDOM_SEED)\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crps_nb_gamma, crps_average_nb_gamma = calculate_crps(idata_bayesian_nb_gamma, actuals=dataY_df, posterior_predictive='obs');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dictionary of parameters used in the model:\n",
    "divergences_nb_gamma = count_divergences(idata_bayesian_nb_gamma)\n",
    "all_dictionaries_nb_gamma = generate_all_dictionaries(\n",
    "    training_data=df, \n",
    "    evaluation_data=df_eval,\n",
    "    data_characteristics_train=data_characteristics_train, \n",
    "    data_characteristics_eval=data_characteristics_eval,\n",
    "    data_transformation=data_transformation,\n",
    "    country_name=country_name, \n",
    "    predictors=predictors, \n",
    "    model_name=model_name_nb_gamma, \n",
    "    prior_specifications=prior_specifications_nb_gamma,\n",
    "    alpha_nb_hyperparameters=(gamma_alpha_nb_alpha, gamma_beta_nb_alpha),\n",
    "    intercept_hyperparameters= (intercept_gaussian_mu, intercept_gaussian_sigma),\n",
    "    beta_hyperparameters=(beta_gaussian_mu, beta_gaussian_sigma),\n",
    "    tuning_iterations=tune_default,\n",
    "    sampling_iterations=draws_default,\n",
    "    target_acceptance_rate=target_accept_default,\n",
    "    chains=no_chains_default,\n",
    "    divergences=divergences_nb_gamma,\n",
    "    sampling_time=sampling_time_nb_gamma, \n",
    "    idata=idata_bayesian_nb_gamma,\n",
    "    crps_score_train=crps_average_nb_gamma,\n",
    "    crps_score_eval=crps_average_nb_gamma)\n",
    "ml_flow_tracking(**all_dictionaries_nb_gamma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Build Bayesian model with Negative Binomial distribution (alpha Gamma distributed) and regression splines (B-Splines)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build the model\n",
    "start = time.time()\n",
    "with pm.Model(coords=COORDS_splines) as bayesian_model_nb_gamma_splines:\n",
    "    # Specify the data\n",
    "    X1 = pm.ConstantData(\"fatality lag t=1\", dataX1, dims=\"obs_idx\")\n",
    "    X2 = pm.ConstantData(\"fatality rolling average t=24\", dataX2, dims=\"obs_idx\")\n",
    "    Y = pm.ConstantData(\"fatility count\", dataY, dims=\"obs_idx\")\n",
    "\n",
    "\n",
    "    # Define the priors for regressors and  negative binomial over-dispersion parameter\n",
    "    intercept = pm.Normal(\"intercept\", mu=intercept_hgaussian_mu, sigma=intercept_hgaussian_sigma)\n",
    "    beta_X1 = pm.Normal(\"slopesX1\", beta_gaussian_mu, beta_gaussian_sigma, dims=\"splines_x1\")\n",
    "    beta_X2 = pm.Normal(\"slopesX2\", beta_gaussian_mu, beta_gaussian_sigma, dims=\"splines_x2\")\n",
    "    alpha = pm.Gamma('alpha', gamma_alpha_nb_alpha, gamma_beta_nb_alpha)\n",
    "    \n",
    "    # Define mean of negative binomial distribution\n",
    "    mu = pm.math.exp(intercept + pm.math.dot(np.asarray(basis_X1, order=\"F\"), beta_X1.T) + pm.math.dot(np.asarray(basis_X2, order=\"F\"), beta_X2.T))\n",
    "\n",
    "    # Define the likelihood\n",
    "    obs = pm.NegativeBinomial(\"obs\", mu=mu, alpha=alpha, observed=Y, dims=\"obs_idx\")\n",
    "\n",
    "    # Note: Optionally define cores and chains\n",
    "    # Run the sampling using the No-U-Turn Sampler (NUTS) for the specified number of samples\n",
    "    idata_bayesian_nb_gamma_splines = pm.sample(draws=draws_default, tune=tune_default, random_seed=RANDOM_SEED, target_accept=target_accept_default)\n",
    "\n",
    "sampling_time_nb_gamma_splines = time.time() - start"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    with bayesian_model_nb_gamma_splines:\n",
    "        ppc_posterior_nb_gamma_splines = pm.sample_posterior_predictive(idata_bayesian_nb_gamma_splines, extend_inferencedata=True, random_seed=RANDOM_SEED)\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)\n",
    "try:\n",
    "    with bayesian_model_nb_gamma_splines:\n",
    "        ppc_prior_nb_gamma_splines = pm.sample_prior_predictive(no_ppc_samples_default, random_seed=RANDOM_SEED)\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crps_nb_gamma_splines, crps_average_nb_gamma_splines = calculate_crps(idata_bayesian_nb_gamma_splines, actuals=dataY_df, posterior_predictive='obs');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dictionary of parameters used in the model:\n",
    "divergences_nb_gamma_splines = count_divergences(idata_bayesian_nb_gamma_splines)\n",
    "all_dictionaries_splines = generate_all_dictionaries(\n",
    "    training_data=df, \n",
    "    evaluation_data=df_eval,\n",
    "    data_characteristics_train=data_characteristics_train, \n",
    "    data_characteristics_eval=data_characteristics_eval,\n",
    "    data_transformation=data_transformation, \n",
    "    country_name=country_name, \n",
    "    predictors=predictors, \n",
    "    model_name=model_name_splines, \n",
    "    prior_specifications=prior_specifications_splines,\n",
    "    alpha_nb_hyperparameters=(gamma_alpha_nb_alpha, gamma_beta_nb_alpha),\n",
    "    intercept_hyperparameters= (intercept_gaussian_mu, intercept_gaussian_sigma),\n",
    "    tuning_iterations=tune_default,\n",
    "    sampling_iterations=draws_default,\n",
    "    target_acceptance_rate=target_accept_default,\n",
    "    chains=no_chains_default,\n",
    "    divergences=divergences_nb_gamma_splines,\n",
    "    sampling_time=sampling_time_nb_gamma_splines, \n",
    "    idata=idata_bayesian_nb_gamma_splines,\n",
    "    crps_score_train=crps_average_nb_gamma_splines,\n",
    "    crps_score_eval=crps_average_nb_gamma_splines)\n",
    "ml_flow_tracking(**all_dictionaries_splines)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Build Bayesian model with Negative Binomial distribution (alpha Gamma distributed) and penalized regression splines with gaussin random walk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# NOTE: WIP: Implementation of penalized regression splines still to be figured out, Version with Gaussian Random Walk\n",
    "# Track the time it takes o sample model\n",
    "start_time = time.time()\n",
    "# Build the model with random walk\n",
    "with pm.Model(coords=COORDS_splines) as bayesian_model_nb_gamma_penalized_splines_grw:\n",
    "    # Specify the data\n",
    "    X1 = pm.ConstantData(\"fatality lag t=1\", dataX1, dims=\"obs_idx\")\n",
    "    X2 = pm.ConstantData(\"fatality rolling average t=24\", dataX2, dims=\"obs_idx\")\n",
    "    Y = pm.ConstantData(\"fatility count\", dataY, dims=\"obs_idx\")\n",
    "\n",
    "\n",
    "    # Define the priors for regressors and  negative binomial over-dispersion parameter\n",
    "    intercept = pm.Normal(\"intercept\", mu=intercept_gaussian_mu, sigma=intercept_gaussian_sigma)\n",
    "\n",
    "    # Define first order random walk to incorporate regularization\n",
    "    tau_X1 = pm.InverseGamma(\"tauX1\", alpha=tauX1_ig_alpha_gaussian_sigma, beta=tauX1_ig_beta_gaussian_sigma)\n",
    "    tau_X2 = pm.InverseGamma(\"tauX2\", alpha=tauX2_ig_alpha_gaussian_sigma, beta=tauX2_ig_beta_gaussian_sigma)\n",
    "\n",
    "    # Define the initial distribution as diffuse prior for the random walk\n",
    "    initial_distribution_X1 = pm.Normal.dist(mu=diffuse_prior_beta_mu, sigma=diffuse_prior_beta_sigma)\n",
    "    initial_distribution_X2 = pm.Normal.dist(mu=diffuse_prior_beta_mu, sigma=diffuse_prior_beta_sigma)\n",
    "    \n",
    "    # Note: Defining the coefficients in a gaussian random walk should incorporate the penalization of differences of consecutive coefficients\n",
    "    beta_X1 = pm.GaussianRandomWalk(\"slopesX1\", sigma=tau_X1, shape=basis_X1.shape[1], init_dist=initial_distribution_X1)\n",
    "    beta_X2 = pm.GaussianRandomWalk(\"slopesX2\", sigma=tau_X2, shape=basis_X2.shape[1], init_dist=initial_distribution_X2)\n",
    "    alpha = pm.Gamma('alpha', gamma_alpha_nb_alpha, gamma_beta_nb_alpha)\n",
    "\n",
    "    # Define mean of negative binomial distribution\n",
    "    # Note: We define the mean of the assumed negative binomial as a linear combination of the basis functions - transformed by a link function to ensure numerical stability\n",
    "    mu = pm.math.exp(intercept + pm.math.dot(np.asarray(basis_X1, order=\"F\"), beta_X1.T) + pm.math.dot(np.asarray(basis_X2, order=\"F\"), beta_X2.T))\n",
    "\n",
    "    # Define the likelihood\n",
    "    # Note: mu is of dimension [N,1] and weighted against the observations in the sampling process\n",
    "    obs = pm.NegativeBinomial(\"obs\", mu=mu, alpha=alpha, observed=Y, dims=\"obs_idx\")\n",
    "\n",
    "    # Run the sampling using the No-U-Turn Sampler (NUTS) for the specified number of samples\n",
    "    idata_bayesian_nb_gamma_penalized_splines_grw = pm.sample(draws=draws_default, tune=tune_default, random_seed=RANDOM_SEED, target_accept=target_accept_default, chains=no_chains_default)\n",
    "    \n",
    "sampling_time_psplines_grw = time.time() - start_time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    with bayesian_model_nb_gamma_penalized_splines_grw:\n",
    "        ppc_posterior_nb_gamma_penalized_splines = pm.sample_posterior_predictive(idata_bayesian_nb_gamma_penalized_splines_grw, extend_inferencedata=True, random_seed=RANDOM_SEED)\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)\n",
    "try:\n",
    "    with bayesian_model_nb_gamma_penalized_splines_grw:\n",
    "        ppc_prior_nb_gamma_penalized_splines = pm.sample_prior_predictive(no_ppc_samples_default, random_seed=RANDOM_SEED)\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate CRPS\n",
    "crps_nb_gamma_penalized_splines_grw, crps_average_nb_gamma_penalized_splines_grw = calculate_crps(idata_bayesian_nb_gamma_penalized_splines_grw, dataY_df, posterior_predictive='obs')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dictionary of parameters used in the model:\n",
    "divergences_splines = count_divergences(idata_bayesian_nb_gamma_splines)\n",
    "all_dictionaries_penalized_splines_grw = generate_all_dictionaries(\n",
    "    training_data=df, \n",
    "    evaluation_data=df_eval,\n",
    "    data_characteristics_train=data_characteristics_train, \n",
    "    data_characteristics_eval=data_characteristics_eval,\n",
    "    data_transformation=data_transformation, \n",
    "    country_name=country_name, \n",
    "    predictors=predictors, \n",
    "    model_name=model_name_penalized_splines_grw, \n",
    "    prior_specifications=prior_specifications_splines,\n",
    "    alpha_nb_hyperparameters=(gamma_alpha_nb_alpha, gamma_beta_nb_alpha),\n",
    "    intercept_hyperparameters= (intercept_gaussian_mu, intercept_gaussian_sigma),\n",
    "    no_spline_coefficients_per_regressor=[no_spline_coefficients_X1, no_spline_coefficients_X2],\n",
    "    tuning_iterations=tune_default,\n",
    "    sampling_iterations=draws_default,\n",
    "    target_acceptance_rate=target_accept_default,\n",
    "    chains=no_chains_default,\n",
    "    divergences=divergences_splines,\n",
    "    sampling_time=sampling_time_psplines_grw, \n",
    "    idata=idata_bayesian_nb_gamma_splines,\n",
    "    crps_score_train=crps_average_nb_gamma_splines,\n",
    "    crps_score_eval=crps_average_nb_gamma_splines)\n",
    "ml_flow_tracking(**all_dictionaries_penalized_splines_grw)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. Build Bayesian model with Negative Binomial distribution (alpha Gamma distributed) and penalized regression splines with penalty potential"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Track sampling time\n",
    "start_time = time.time()\n",
    "# NOTE: WIP: Implementation of penalized regression splines still to be figured out, Version with Penalty deduction in likeihood\n",
    "\n",
    "with pm.Model(coords=COORDS_penalized_splines_penalty) as bayesian_model_nb_gamma_penalized_splines_penalty:\n",
    "    # Specify the data\n",
    "    X1 = pm.ConstantData(\"fatality lag t=1\", dataX1, dims=\"obs_idx\")\n",
    "    X2 = pm.ConstantData(\"fatality rolling average t=24\", dataX2, dims=\"obs_idx\")\n",
    "    Y = pm.ConstantData(\"fatility count\", dataY, dims=\"obs_idx\")\n",
    "\n",
    "    # Define the priors for regressors and negative binomial over-dispersion parameter\n",
    "    intercept = pm.Normal(\"intercept\", mu=intercept_gaussian_mu, sigma=intercept_gaussian_sigma)\n",
    "\n",
    "    # Define first order random walk to incorporate regularization\n",
    "    tau_X1 = pm.InverseGamma(\"tauX1\", alpha=tauX1_ig_alpha_gaussian_sigma, beta=tauX1_ig_beta_gaussian_sigma)\n",
    "    tau_X2 = pm.InverseGamma(\"tauX2\", alpha=tauX2_ig_alpha_gaussian_sigma, beta=tauX2_ig_beta_gaussian_sigma)\n",
    "\n",
    "    # Create deterministic variables for squared taus\n",
    "    tau_X1_squared = pm.Deterministic(\"tau_X1_squared\", tau_X1 ** 2)\n",
    "    tau_X2_squared = pm.Deterministic(\"tau_X2_squared\", tau_X2 ** 2)\n",
    "    \n",
    "    # Note: If this does reflect our intended model remains to be questioned\n",
    "    # Introduce diffuse priors for the first regression coefficients of the splines with broad gaussian distributions\n",
    "    beta_X1_0 = pm.Normal(\"slopesX1_0\", diffuse_prior_beta_mu, diffuse_prior_beta_sigma)\n",
    "    beta_X2_0 = pm.Normal(\"slopesX2_0\", diffuse_prior_beta_mu, diffuse_prior_beta_sigma)\n",
    "    \n",
    "    # Define remaining betas as normal variables with standard deviation tau\n",
    "    beta_X1_rest = pm.Normal(\"slopesX1_rest\", beta_gaussian_mu, tau_X1, dims=\"splines_x1_rest\")\n",
    "    beta_X2_rest = pm.Normal(\"slopesX2_rest\", beta_gaussian_mu, tau_X2, dims=\"splines_x2_rest\")\n",
    "    \n",
    "    # Concatenate the flat priors to the beginning of the coefficients\n",
    "    beta_X1 = pm.Deterministic(\"slopesX1\", pm.math.concatenate([[beta_X1_0], beta_X1_rest]))\n",
    "    beta_X2 = pm.Deterministic(\"slopesX2\", pm.math.concatenate([[beta_X2_0], beta_X2_rest]))\n",
    "\n",
    "\n",
    "    alpha = pm.Gamma('alpha', gamma_alpha_nb_alpha, gamma_beta_nb_alpha)\n",
    "\n",
    "    # Define mean of negative binomial distribution\n",
    "    mu = pm.math.exp(intercept + pm.math.dot(np.asarray(basis_X1, order=\"F\"), beta_X1.T) + pm.math.dot(np.asarray(basis_X2, order=\"F\"), beta_X2.T))\n",
    "\n",
    "    # Define the likelihood\n",
    "    obs = pm.NegativeBinomial(\"obs\", mu=mu, alpha=alpha, observed=Y, dims=\"obs_idx\")\n",
    "\n",
    "    # Define the penalty term and incorporate it using pm.Potential\n",
    "    penalty_term_X1 = pm.math.dot(beta_X1.T, pm.math.dot(K_X1, beta_X1)) / (2 * tau_X1_squared)\n",
    "    penalty_X1 = pm.Potential(\"penalty_X1\", -penalty_term_X1)\n",
    "\n",
    "    penalty_term_X2 = pm.math.dot(beta_X2.T, pm.math.dot(K_X2, beta_X2)) / (2 * tau_X2_squared)\n",
    "    penalty_X2 = pm.Potential(\"penalty_X2\", -penalty_term_X2)\n",
    "\n",
    "    # Run the sampling using the No-U-Turn Sampler (NUTS) for the specified number of samples\n",
    "    idata_bayesian_nb_gamma_penalized_splines_penalty = pm.sample(draws=draws_default, tune=tune_default, random_seed=RANDOM_SEED, target_accept=target_accept_default, chains=no_chains_default)\n",
    "\n",
    "sampling_time_psplines_penalty = time.time() - start_time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    with bayesian_model_nb_gamma_penalized_splines_penalty:\n",
    "        ppc_posterior_nb_gamma_penalized_splines_penalty = pm.sample_posterior_predictive(idata_bayesian_nb_gamma_penalized_splines_penalty, extend_inferencedata=True, random_seed=RANDOM_SEED)\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)\n",
    "try:\n",
    "    with bayesian_model_nb_gamma_penalized_splines_penalty:\n",
    "        ppc_prior_nb_gamma_penalized_splines_penalty = pm.sample_prior_predictive(no_ppc_samples_default, random_seed=RANDOM_SEED)\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crps_nb_gamma_penalized_splines_penalty, crps_average_nb_gamma_penalized_splines_penalty = calculate_crps(idata_bayesian_nb_gamma_penalized_splines_penalty, actuals=dataY_df, posterior_predictive='obs');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dictionary of parameters used in the model:\n",
    "divergences_psplines_penalty = count_divergences(idata_bayesian_nb_gamma_penalized_splines_penalty)\n",
    "all_dictionaries = generate_all_dictionaries(\n",
    "    training_data=df, \n",
    "    evaluation_data=df_eval,\n",
    "    data_characteristics_train=data_characteristics_train, \n",
    "    data_characteristics_eval=data_characteristics_eval, \n",
    "    data_transformation=data_transformation, \n",
    "    country_name=country_name, \n",
    "    predictors=predictors, \n",
    "    model_name=model_name_penalized_splines_penalty, \n",
    "    prior_specifications=prior_specifications_penalized_splines_penalty, \n",
    "    no_spline_coefficients_per_regressor=[no_spline_coefficients_X1, no_spline_coefficients_X2],\n",
    "    tau_hyperparameters=[(tauX1_ig_alpha_gaussian_sigma, tauX1_ig_beta_gaussian_sigma), (tauX2_ig_alpha_gaussian_sigma, tauX2_ig_beta_gaussian_sigma)],\n",
    "    alpha_nb_hyperparameters=(gamma_alpha_nb_alpha, gamma_beta_nb_alpha),\n",
    "    intercept_hyperparameters= (intercept_gaussian_mu, intercept_gaussian_sigma),\n",
    "    tuning_iterations=tune_default,\n",
    "    sampling_iterations=draws_default,\n",
    "    target_acceptance_rate=target_accept_default,\n",
    "    chains=no_chains_default,\n",
    "    divergences=divergences_psplines_penalty,\n",
    "    sampling_time=sampling_time_psplines_penalty, \n",
    "    idata=idata_bayesian_nb_gamma_penalized_splines_penalty,\n",
    "    crps_score_train=crps_average_nb_gamma_penalized_splines_penalty,\n",
    "    crps_score_eval=crps_average_nb_gamma_penalized_splines_penalty)\n",
    "ml_flow_tracking(**all_dictionaries)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**7. Explore other ways to build Bayesian model with Negative Binomial distribution (alpha Gamma distributed) and penalized regression splines**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # NOTE: WIP: Implementation of penalized regression splines still to be figured out, Version with Penalty Matrix\n",
    "# # Build the model with penalty matrix\n",
    "# with pm.Model(coords=COORDS_splines) as bayesian_model_nb_gamma_penalized_splines_penalty_K:\n",
    "#     # Specify the data\n",
    "#     X1 = pm.ConstantData(\"fatality lag t=1\", dataX1, dims=\"obs_idx\")\n",
    "#     X2 = pm.ConstantData(\"fatality rolling average t=24\", dataX2, dims=\"obs_idx\")\n",
    "#     Y = pm.ConstantData(\"fatility count\", dataY, dims=\"obs_idx\")\n",
    "# \n",
    "# \n",
    "#     # Define the priors for regressors and  negative binomial over-dispersion parameter\n",
    "#     intercept = pm.HalfNormal(\"intercept\", sigma=0.1)\n",
    "# \n",
    "#     # Define first order random walk to incorporate regularization\n",
    "#     tau_X1 = pm.InverseGamma(\"tauX1\", alpha=a_X1, beta=b_X1)\n",
    "#     tau_X2 = pm.InverseGamma(\"tauX2\", alpha=a_X2, beta=b_X2)\n",
    "# \n",
    "#     # Calculate covariance matrices\n",
    "#     cov_X1 = at.dot(at.nlinalg.matrix_inverse(K_X1), at.pow(tau_X1, 2))\n",
    "#     cov_X2 = at.dot(at.nlinalg.matrix_inverse(K_X2), at.pow(tau_X2, 2))\n",
    "#     \n",
    "#     # Compute cholesky decompositions of covariance matrices\n",
    "#     chol_X1 = at.slinalg.cholesky(cov_X1)\n",
    "#     chol_X2 = at.slinalg.cholesky(cov_X2)\n",
    "# \n",
    "#     # Define multivariate normal distributions\n",
    "#     beta_X1 = pm.MvNormal(\"slopesX1\", mu=at.zeros(no_X1_coefficients), chol=chol_X1, shape=no_X1_coefficients)\n",
    "#     beta_X2 = pm.MvNormal(\"slopesX2\", mu=at.zeros(no_X2_coefficients), chol=chol_X2, shape=no_X2_coefficients)\n",
    "# \n",
    "#     alpha = pm.Gamma('alpha', 0.1, 0.1)\n",
    "# \n",
    "#     # Note: Possibly an option to change to pm.Deterministic\n",
    "#     # Define mean of negative binomial distribution\n",
    "#     mu = pm.math.exp(intercept + pm.math.dot(at.as_tensor_variable(basis_X1), beta_X1.T) + pm.math.dot(at.as_tensor_variable(basis_X2),beta_X2.T))\n",
    "# \n",
    "#     # Define the likelihood\n",
    "#     obs = pm.NegativeBinomial(\"obs\", mu=mu, alpha=alpha, observed=Y, dims=\"obs_idx\")\n",
    "# \n",
    "#     # Note: Optionally define cores and chains\n",
    "#     # Run the sampling using the No-U-Turn Sampler (NUTS) for the specified number of samples\n",
    "#     idata_bayesian_nb_gamma_penalized_splines_penalty_K = pm.sample(draws=1000, tune=2000, random_seed=RANDOM_SEED, target_accept=0.95)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# try:\n",
    "#     with bayesian_model_nb_gamma_penalized_splines_penalty_K:\n",
    "#         ppc_posterior_nb_gamma_penalized_splines_penalty_K = pm.sample_posterior_predictive(idata_bayesian_nb_gamma_penalized_splines_penalty_K, extend_inferencedata=True, random_seed=RANDOM_SEED)\n",
    "# except Exception as error:\n",
    "#     print(\"Error\", error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# try:\n",
    "#     with bayesian_model_nb_gamma_penalized_splines_penalty_K:\n",
    "#         ppc_prior_nb_gamma_penalized_splines_penalty_K = pm.sample_prior_predictive(no_ppc_samples_default, random_seed=RANDOM_SEED)\n",
    "# except Exception as error:\n",
    "#     print(\"Error\", error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Note: Pending suggestion of ChatGPT to use tau for covariance altough it's part of the sampling process itself\n",
    "# # Create a covariance matrix using Theano operations\n",
    "# tau_X1_squared = tau_X1**2\n",
    "# tau_X1_squared_diag = tt.diag(tau_X1_squared)\n",
    "# cov_X1 = tt.dot(np.linalg.inv(K_X1), tau_X1_squared_diag)\n",
    "# \n",
    "# tau_X2_squared = tau_X2**2\n",
    "# tau_X2_squared_diag = tt.diag(tau_X2_squared)\n",
    "# cov_X2 = tt.dot(np.linalg.inv(K_X2), tau_X2_squared_diag)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## III.2 PyStan Model Building"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 1: Negative Binomial distribution (alpha Gamma distributed) and penalized splines via \"manual\" random walk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Likelihood: negative binomial\n",
    "Splines: yes\n",
    "Penalization: first order random walk\n",
    "Implementation: diffuse prior + manually"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "definition_stan_model_nb_gamma_penalized_splines_grw = \"\"\"\n",
    "data {\n",
    "    int<lower=0> N;                      // Number of data points\n",
    "    int<lower=1> num_basis_X1;           // Number of coefficients for P-splines for X1\n",
    "    int<lower=1> num_basis_X2;           // Number of coefficients for P-splines for X2\n",
    "\n",
    "    matrix[N, num_basis_X1] basis_X1;    // Basis matrix for regressor X1\n",
    "    matrix[N, num_basis_X2] basis_X2;    // Basis matrix for regressor X2\n",
    "    int Y[N];                            // Observed Target variable\n",
    "    \n",
    "    real a_tau_X1;                       // Shape parameter for InverseGamma of tau for X1\n",
    "    real b_tau_X1;                       // Scale parameter for InverseGamma of tau for X1\n",
    "    real a_tau_X2;                       // Shape parameter for InverseGamma of tau for X2\n",
    "    real b_tau_X2;                       // Scale parameter for InverseGamma of tau for X2\n",
    "    real a_alpha;                        // Shape parameter for Gamma of alpha\n",
    "    real b_alpha;                        // Scale parameter for Gamma of alpha\n",
    "    real diffuse_prior_mu;                    // Mean of spline coefficient diffuse prior\n",
    "    real<lower=0> diffuse_prior_sigma;                 // Standard deviation of spline coefficient diffuse prior\n",
    "    real intercept_mu;                        // Mean of intercept prior\n",
    "    real<lower=1> intercept_sigma;                     // Standard deviation of intercept prior\n",
    "    \n",
    "    int<lower=0> N_eval;                 // Number of data points for evaluation\n",
    "    matrix[N_eval, num_basis_X1] basis_X1_eval;    // Basis matrix for regressor X1_eval\n",
    "    matrix[N_eval, num_basis_X2] basis_X2_eval;    // Basis matrix for regressor X2_eval\n",
    "    int Y_eval[N_eval];                            // Observed Target variable for evaluation\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    vector[num_basis_X1] spline_coefficients_X1;\n",
    "    vector[num_basis_X2] spline_coefficients_X2;\n",
    "    real intercept;\n",
    "    real<lower=0> tau_X1;\n",
    "    real<lower=0> tau_X2;\n",
    "    real<lower=0> alpha;\n",
    "}\n",
    "\n",
    "transformed parameters {\n",
    "    vector[N] mu;\n",
    "    mu = exp(intercept + to_vector(basis_X1 * spline_coefficients_X1) + to_vector(basis_X2 * spline_coefficients_X2));\n",
    "}\n",
    "\n",
    "model {\n",
    "    intercept ~ normal(intercept_mu, intercept_sigma);\n",
    "    tau_X1 ~ inv_gamma(a_tau_X1, b_tau_X1);\n",
    "    tau_X2 ~ inv_gamma(a_tau_X2, b_tau_X2);\n",
    "\n",
    "    spline_coefficients_X1[1] ~ normal(diffuse_prior_mu, diffuse_prior_sigma);      // diffuse prior for first coefficient\n",
    "    for (i in 2:num_basis_X1) {                     // random walk prior for coefficients\n",
    "        spline_coefficients_X1[i] ~ normal(spline_coefficients_X1[i-1], tau_X1);\n",
    "    }\n",
    "    \n",
    "    spline_coefficients_X2[1] ~ normal(diffuse_prior_mu, diffuse_prior_sigma);      // diffuse prior for first coefficient\n",
    "    for (i in 2:num_basis_X2) {                     // random walk prior for coefficients\n",
    "        spline_coefficients_X2[i] ~ normal(spline_coefficients_X2[i-1], tau_X2);\n",
    "    }\n",
    "    \n",
    "    alpha ~ gamma(0.1, 0.1);\n",
    "    Y ~ neg_binomial_2(mu, alpha);\n",
    "}\n",
    "generated quantities {\n",
    "    int y_pred_train[N];\n",
    "    int y_pred_eval[N_eval];\n",
    "    vector[N_eval] mu_eval;\n",
    "    \n",
    "    mu_eval = exp(intercept + to_vector(basis_X1_eval * spline_coefficients_X1) + to_vector(basis_X2_eval * spline_coefficients_X2));\n",
    "    for (n in 1:N) {\n",
    "        y_pred_train[n] = neg_binomial_2_rng(mu[n], alpha);\n",
    "    }\n",
    "    for (n_eval in 1:N_eval) {\n",
    "        y_pred_eval[n_eval] = neg_binomial_2_rng(mu_eval[n_eval], alpha);\n",
    "    }\n",
    "}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:27:21.098780200Z",
     "start_time": "2023-09-06T16:27:20.949205200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_230c9f40206178d84ce58224164f070c NOW.\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "stan_model_nb_gamma_penalized_splines_grw = pystan.StanModel(model_code=definition_stan_model_nb_gamma_penalized_splines_grw)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:28:22.704228Z",
     "start_time": "2023-09-06T16:27:20.960150900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "stan_data_nb_gamma_penalized_splines_grw = {\n",
    "    \"N\": len(dataY),\n",
    "    \"num_basis_X1\": no_spline_coefficients_X1,\n",
    "    \"num_basis_X2\": no_spline_coefficients_X2,\n",
    "    \"basis_X1\": np.asarray(basis_X1, order=\"F\"),\n",
    "    \"basis_X2\": np.asarray(basis_X2, order=\"F\"),\n",
    "    \"Y\": dataY.astype(int),\n",
    "    \"a_tau_X1\": tauX1_ig_alpha_gaussian_sigma,\n",
    "    \"b_tau_X1\": tauX1_ig_beta_gaussian_sigma,\n",
    "    \"a_tau_X2\": tauX2_ig_alpha_gaussian_sigma,\n",
    "    \"b_tau_X2\": tauX2_ig_beta_gaussian_sigma,\n",
    "    \"a_alpha\": gamma_alpha_nb_alpha,\n",
    "    \"b_alpha\": gamma_beta_nb_alpha,\n",
    "    \"diffuse_prior_mu\": diffuse_prior_beta_mu,\n",
    "    \"diffuse_prior_sigma\": diffuse_prior_beta_sigma,\n",
    "    \"intercept_mu\": intercept_gaussian_mu,\n",
    "    \"intercept_sigma\": intercept_gaussian_sigma,\n",
    "    \"N_eval\": len(dataY_eval),\n",
    "    \"basis_X1_eval\": np.asarray(basis_X1_eval, order=\"F\"),\n",
    "    \"basis_X2_eval\": np.asarray(basis_X2_eval, order=\"F\"),\n",
    "    \"Y_eval\": dataY_eval.astype(int)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:30:01.179791Z",
     "start_time": "2023-09-06T16:30:01.052352500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:n_eff / iter below 0.001 indicates that the effective sample size has likely been overestimated\n",
      "WARNING:pystan:Rhat above 1.1 or below 0.9 indicates that the chains very likely have not mixed\n",
      "WARNING:pystan:248 of 2000 iterations ended with a divergence (12.4 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.95 to remove the divergences.\n",
      "WARNING:pystan:1484 of 2000 iterations saturated the maximum tree depth of 10 (74.2 %)\n",
      "WARNING:pystan:Run again with max_treedepth larger than 10 to avoid saturation\n",
      "WARNING:pystan:Chain 3: E-BFMI = 0.195\n",
      "WARNING:pystan:Chain 4: E-BFMI = 0.117\n",
      "WARNING:pystan:E-BFMI below 0.2 indicates you may need to reparameterize your model\n"
     ]
    }
   ],
   "source": [
    "# Sampling from the model\n",
    "start_time = time.time()\n",
    "stan_model_nb_gamma_penalized_splines_grw_sampling = stan_model_nb_gamma_penalized_splines_grw.sampling(data=stan_data_nb_gamma_penalized_splines_grw, iter=draws_default, chains=no_chains_default, seed=RANDOM_SEED, control={\"adapt_delta\": target_accept_default})\n",
    "sampling_time_nb_gamma_penalized_splines_grw_stan = time.time() - start_time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:34:05.879661500Z",
     "start_time": "2023-09-06T16:30:02.616251500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "idata_bayesian_nb_gamma_penalized_splines_grw_stan = az.from_pystan(\n",
    "    posterior=stan_model_nb_gamma_penalized_splines_grw_sampling,\n",
    "    posterior_predictive=[\"y_pred_train\", \"y_pred_eval\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:34:58.306960600Z",
     "start_time": "2023-09-06T16:34:58.199697300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "'temp_data/idata_bayesian_nb_gamma_penalized_splines_grw_stan.nc'"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: possibility to export idata as example object\n",
    "# idata_bayesian_nb_gamma_penalized_splines_grw_stan.to_netcdf(\"temp_data/idata_bayesian_nb_gamma_penalized_splines_grw_stan.nc\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:35:09.648183500Z",
     "start_time": "2023-09-06T16:35:09.320513600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Note: possibility to import idata as example object\n",
    "# idata_bayesian_nb_gamma_penalized_splines_grw_stan = az.from_netcdf(\"temp_data/idata_bayesian_nb_gamma_penalized_splines_grw_stan.nc\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Uwe Drauz\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\CRPS\\CRPS.py:147: RuntimeWarning: overflow encountered in long_scalars\n",
      "  self.__delta_fc = np.array([all_mem[n+1] - all_mem[n] for n in range(len(all_mem)-1)] + list(np.zeros(1)), dtype=object)\n",
      "C:\\Users\\Uwe Drauz\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\CRPS\\CRPS.py:141: RuntimeWarning: overflow encountered in long_scalars\n",
      "  self.__delta_fc = np.array([all_mem[n+1] - all_mem[n] for n in range(len(all_mem)-1)], dtype=object)\n",
      "C:\\Users\\Uwe Drauz\\anaconda3\\envs\\bachelor_thesis_pystan_clean\\lib\\site-packages\\CRPS\\CRPS.py:157: RuntimeWarning: overflow encountered in long_scalars\n",
      "  delta_fc.append(self.fc[f+1] - self.fc[f])\n"
     ]
    }
   ],
   "source": [
    "crps_train_nb_gamma_penalized_splines_grw_stan, crps_train_average_nb_gamma_penalized_splines_grw_stan = calculate_crps(idata_bayesian_nb_gamma_penalized_splines_grw_stan, dataY_df, posterior_predictive=\"y_pred_train\")\n",
    "crps_eval_nb_gamma_penalized_splines_grw_stan, crps_eval_average_nb_gamma_penalized_splines_grw_stan = calculate_crps(idata_bayesian_nb_gamma_penalized_splines_grw_stan, dataY_eval_df, posterior_predictive=\"y_pred_eval\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:35:24.613122300Z",
     "start_time": "2023-09-06T16:35:17.963415300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dictionary of parameters used in the model:\n",
    "divergences_splines_stan = count_divergences(idata_bayesian_nb_gamma_penalized_splines_grw_stan)\n",
    "all_dictionaries_penalized_splines_grw = generate_all_dictionaries(\n",
    "    training_data=df,\n",
    "    evaluation_data=df_eval,\n",
    "    data_characteristics_train=data_characteristics_train, \n",
    "    data_characteristics_eval=data_characteristics_eval,\n",
    "    data_transformation=data_transformation, \n",
    "    country_name=country_name, \n",
    "    predictors=predictors, \n",
    "    model_name=model_name_penalized_splines_grw, \n",
    "    prior_specifications=prior_specifications_penalized_splines_grw,\n",
    "    alpha_nb_hyperparameters=(gamma_alpha_nb_alpha, gamma_beta_nb_alpha),\n",
    "    intercept_hyperparameters= (intercept_gaussian_mu, intercept_gaussian_sigma),\n",
    "    no_spline_coefficients_per_regressor=[no_spline_coefficients_X1, no_spline_coefficients_X2],\n",
    "    tuning_iterations=tune_default,\n",
    "    sampling_iterations=draws_default,\n",
    "    target_acceptance_rate=target_accept_default,\n",
    "    chains=no_chains_default,\n",
    "    divergences=divergences_splines_stan,\n",
    "    sampling_time=sampling_time_nb_gamma_penalized_splines_grw_stan, \n",
    "    idata=idata_bayesian_nb_gamma_penalized_splines_grw_stan,\n",
    "    crps_score_train=crps_train_average_nb_gamma_penalized_splines_grw_stan,\n",
    "    crps_score_eval=crps_eval_average_nb_gamma_penalized_splines_grw_stan)\n",
    "ml_flow_tracking(**all_dictionaries_penalized_splines_grw)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model 2: Negative Binomial distribution (alpha Gamma distributed) and penalized splines via multivariate distribution using penalty matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "definition_stan_model_nb_gamma_penalized_splines_multivariate = \"\"\"\n",
    "data {\n",
    "    int<lower=1> N;          // Number of data points\n",
    "    int<lower=1> num_basis_X1;  // Number of coefficients for P-splines for X1\n",
    "    int<lower=1> num_basis_X2;  // Number of coefficients for P-splines for X2\n",
    "\n",
    "    matrix[N, num_basis_X1] basis_X1;  // Basis matrix for regressor X1\n",
    "    matrix[N, num_basis_X2] basis_X2;  // Basis matrix for regressor X2\n",
    "    int Y[N];             // Observed Target variable\n",
    "    \n",
    "    matrix[num_basis_X1, num_basis_X1] K_X1;  // Penalty matrix for X1\n",
    "    matrix[num_basis_X2, num_basis_X2] K_X2;  // Penalty matrix for X2\n",
    "    \n",
    "    real a_tau_X1;              // Shape parameter for InverseGamma of tau for X1\n",
    "    real b_tau_X1;              // Scale parameter for InverseGamma of tau for X1\n",
    "    real a_tau_X2;            // Shape parameter for InverseGamma of tau for X2\n",
    "    real b_tau_X2;            // Scale parameter for InverseGamma of tau for X2\n",
    "    real a_alpha;             // Shape parameter for Gamma of alpha\n",
    "    real b_alpha;             // Scale parameter for Gamma of alpha\n",
    "    real intercept_mu;        // Mean of intercept prior\n",
    "    real<lower=0> intercept_sigma;     // Standard deviation of intercept prior\n",
    "    \n",
    "    int<lower=0> N_eval;                 // Number of data points for evaluation\n",
    "    matrix[N_eval, num_basis_X1] basis_X1_eval;    // Basis matrix for regressor X1_eval\n",
    "    matrix[N_eval, num_basis_X2] basis_X2_eval;    // Basis matrix for regressor X2_eval\n",
    "    int Y_eval[N_eval];                            // Observed Target variable for evaluation\n",
    "}\n",
    "\n",
    "parameters {\n",
    "    real intercept;                           // regression intercept\n",
    "    vector[num_basis_X1] spline_coefficients_X1; // Spline coefficients for X1\n",
    "    vector[num_basis_X2] spline_coefficients_X2; // Spline coefficients for X2\n",
    "    real<lower=0> tau_X1;                        // Precision parameter for Gaussian random walk for X1\n",
    "    real<lower=0> tau_X2;                      // Precision parameter for Gaussian random walk for X2\n",
    "    real<lower=0> alpha;                      // Negative Binomial dispersion parameter\n",
    "}\n",
    "transformed parameters {\n",
    "    vector[N] mu; // Mean of Negative Binomial distribution\n",
    "    matrix[num_basis_X1, num_basis_X1] precision_matrix_X1;  // precision matrix for X1\n",
    "    matrix[num_basis_X2, num_basis_X2] precision_matrix_X2;  // precision matrix for X2\n",
    "\n",
    "    mu = exp(intercept + basis_X1 * spline_coefficients_X1 + basis_X2 * spline_coefficients_X2); //taking exponential as a link function of a GAM\n",
    "\n",
    "    precision_matrix_X1 = K_X1 / pow(tau_X1, 2);  // Defining precision matrix for X1\n",
    "    precision_matrix_X2 = K_X2 / pow(tau_X2, 2);  // Defining precision matrix for X2\n",
    "}\n",
    "\n",
    "model {\n",
    "    // Priors\n",
    "    intercept ~ normal(0, 10);               // Diffuse prior for intercept\n",
    "    tau_X1 ~ inv_gamma(a_tau_X1, b_tau_X1);  // Inverse Gamma prior for tau_X1\n",
    "    tau_X2 ~ inv_gamma(a_tau_X2, b_tau_X2);  // Inverse Gamma prior for tau_X2\n",
    "    alpha ~ gamma(0.1, 0.1);              // Gamma prior for alpha\n",
    "    \n",
    "    spline_coefficients_X1 ~ multi_normal_prec(rep_vector(0, num_basis_X1), precision_matrix_X1);  // Prior for spline coefficients for X1\n",
    "    spline_coefficients_X2 ~ multi_normal_prec(rep_vector(0, num_basis_X2), precision_matrix_X2);  // Prior for spline coefficients for X2\n",
    "\n",
    "    // Likelihood\n",
    "    Y ~ neg_binomial_2(mu, alpha);\n",
    "}\n",
    "generated quantities{\n",
    "    int y_pred_train[N];\n",
    "    int y_pred_eval[N_eval];\n",
    "    vector[N_eval] mu_eval;\n",
    "    \n",
    "    mu_eval = exp(intercept + basis_X1_eval * spline_coefficients_X1 + basis_X2_eval * spline_coefficients_X2);\n",
    "    for (n in 1:N) {\n",
    "        y_pred_train[n] = neg_binomial_2_rng(mu[n], alpha);\n",
    "    }\n",
    "    for (n_eval in 1:N_eval) {\n",
    "        y_pred_eval[n_eval] = neg_binomial_2_rng(mu_eval[n_eval], alpha);\n",
    "    }\n",
    "}   \n",
    "\"\"\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:44:58.358598900Z",
     "start_time": "2023-09-06T16:44:58.201021400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_7e45e14c9e53ce0eca39a351c9e98e63 NOW.\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "stan_model_nb_gamma_penalized_splines_multivariate = pystan.StanModel(model_code=definition_stan_model_nb_gamma_penalized_splines_multivariate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:46:06.535391Z",
     "start_time": "2023-09-06T16:45:03.393103400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "stan_data_nb_gamma_penalized_splines_multivariate = {\n",
    "    \"N\": len(dataY),\n",
    "    \"num_basis_X1\": no_spline_coefficients_X1,\n",
    "    \"num_basis_X2\": no_spline_coefficients_X2,\n",
    "    \"basis_X1\": np.asarray(basis_X1, order=\"F\"),\n",
    "    \"basis_X2\": np.asarray(basis_X2, order=\"F\"),\n",
    "    \"Y\": dataY.astype(int),\n",
    "    \"K_X1\": K_X1,\n",
    "    \"K_X2\": K_X2,\n",
    "    \"a_tau_X1\": tauX1_ig_alpha_gaussian_sigma,\n",
    "    \"b_tau_X1\": tauX1_ig_beta_gaussian_sigma,\n",
    "    \"a_tau_X2\": tauX2_ig_alpha_gaussian_sigma,\n",
    "    \"b_tau_X2\": tauX2_ig_beta_gaussian_sigma,\n",
    "    \"a_alpha\": gamma_alpha_nb_alpha,\n",
    "    \"b_alpha\": gamma_beta_nb_alpha,\n",
    "    \"intercept_mu\": intercept_gaussian_mu,\n",
    "    \"intercept_sigma\": intercept_gaussian_sigma,\n",
    "    \"N_eval\": len(dataY_eval),\n",
    "    \"basis_X1_eval\": np.asarray(basis_X1_eval, order=\"F\"),\n",
    "    \"basis_X2_eval\": np.asarray(basis_X2_eval, order=\"F\"),\n",
    "    \"Y_eval\": dataY_eval.astype(int)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:46:31.314066300Z",
     "start_time": "2023-09-06T16:46:31.160442100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "WARNING:pystan:62 of 2000 iterations ended with a divergence (3.1 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.95 to remove the divergences.\n",
      "WARNING:pystan:1938 of 2000 iterations saturated the maximum tree depth of 10 (96.9 %)\n",
      "WARNING:pystan:Run again with max_treedepth larger than 10 to avoid saturation\n"
     ]
    }
   ],
   "source": [
    "# Sampling from the model\n",
    "start_time = time.time()\n",
    "stan_model_nb_gamma_penalized_splines_multivariate_sampling = stan_model_nb_gamma_penalized_splines_multivariate.sampling(data=stan_data_nb_gamma_penalized_splines_multivariate, iter=draws_default, chains=no_chains_default, seed=RANDOM_SEED, control={\"adapt_delta\": target_accept_default})\n",
    "sampling_time_nb_gamma_penalized_splines_multivariate_stan = time.time() - start_time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:51:57.666827800Z",
     "start_time": "2023-09-06T16:46:36.475491300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "idata_bayesian_nb_gamma_penalized_splines_multivariate_stan = az.from_pystan(\n",
    "    posterior=stan_model_nb_gamma_penalized_splines_multivariate_sampling,\n",
    "    posterior_predictive=[\"y_pred_train\", \"y_pred_eval\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:52:25.793025400Z",
     "start_time": "2023-09-06T16:52:25.663372200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "'temp_data/idata_bayesian_nb_gamma_penalized_splines_multivariate_stan.nc'"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: possibility to export idata as example object\n",
    "# idata_bayesian_nb_gamma_penalized_splines_multivariate_stan.to_netcdf(\"temp_data/idata_bayesian_nb_gamma_penalized_splines_multivariate_stan.nc\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:55:59.731706600Z",
     "start_time": "2023-09-06T16:55:59.371616400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Note: possibility to import idata as example object\n",
    "# idata_bayesian_nb_gamma_penalized_splines_multivariate_stan = az.from_netcdf(\"temp_data/idata_bayesian_nb_gamma_penalized_splines_multivariate_stan.nc\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "crps_train_nb_gamma_penalized_splines_multivariate_stan, crps_train_average_nb_gamma_penalized_splines_multivariate_stan = calculate_crps(idata_bayesian_nb_gamma_penalized_splines_multivariate_stan, dataY_df, posterior_predictive=\"y_pred_train\")\n",
    "crps_eval_nb_gamma_penalized_splines_multivariate_stan, crps_eval_average_nb_gamma_penalized_splines_multivariate_stan = calculate_crps(idata_bayesian_nb_gamma_penalized_splines_multivariate_stan, dataY_eval_df, posterior_predictive=\"y_pred_eval\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T16:52:38.461885400Z",
     "start_time": "2023-09-06T16:52:33.615083600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dictionary of parameters used in the model:\n",
    "divergences_psplines_penalty_stan = count_divergences(idata_bayesian_nb_gamma_penalized_splines_penalty)\n",
    "all_dictionaries = generate_all_dictionaries(\n",
    "    training_data=df,\n",
    "    evaluation_data=df_eval,\n",
    "    data_characteristics_train=data_characteristics_train,\n",
    "    data_characteristics_eval=data_characteristics_eval,\n",
    "    data_transformation=data_transformation,\n",
    "    country_name=country_name,\n",
    "    predictors=predictors,\n",
    "    model_name=model_name_penalized_splines_penalty,\n",
    "    prior_specifications=prior_specifications_penalized_splines_penalty,\n",
    "    no_spline_coefficients_per_regressor=[no_spline_coefficients_X1, no_spline_coefficients_X2],\n",
    "    tau_hyperparameters=[(tauX1_ig_alpha_gaussian_sigma, tauX1_ig_beta_gaussian_sigma),\n",
    "                         (tauX2_ig_alpha_gaussian_sigma, tauX2_ig_beta_gaussian_sigma)],\n",
    "    alpha_nb_hyperparameters=(gamma_alpha_nb_alpha, gamma_beta_nb_alpha),\n",
    "    intercept_hyperparameters=(intercept_gaussian_mu, intercept_gaussian_sigma),\n",
    "    tuning_iterations=tune_default,\n",
    "    sampling_iterations=draws_default,\n",
    "    target_acceptance_rate=target_accept_default,\n",
    "    chains=no_chains_default,\n",
    "    divergences=divergences_psplines_penalty,\n",
    "    sampling_time=sampling_time_psplines_penalty,\n",
    "    idata=idata_bayesian_nb_gamma_penalized_splines_penalty,\n",
    "    crps_score_train=crps_average_nb_gamma_penalized_splines_penalty,\n",
    "    crps_score_eval=crps_average_nb_gamma_penalized_splines_penalty)\n",
    "ml_flow_tracking(**all_dictionaries)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## IV.1 PyMc Model Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T11:30:37.829982Z",
     "start_time": "2023-07-17T11:30:37.779118300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.Model with Gaussian distribution for likelihood"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify which model should be examined\n",
    "model = bayesian_model_gauss\n",
    "idata = idata_bayesian_gauss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Visualisations**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_trace(idata, figsize=(10, 9))\n",
    "az.plot_trace(idata, combined=True, figsize=(10, 9))\n",
    "az.plot_posterior(idata, figsize=(10, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_ppc(idata, num_pp_samples=100, kind=\"scatter\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Summary Statistics**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.summary(idata.posterior, var_names=[\"intercept\", \"slopes\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.summary(idata.posterior, var_names=[\"sigma\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CRPS evaluation**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.Model with negative binomial distribution for likelihood (alpha half normal distributed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify which model should be examined\n",
    "model = bayesian_model_nb_half_normal\n",
    "idata = idata_bayesian_nb_half_normal"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Visualisations**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting trace with parameters transformed back to original scale\n",
    "# Plotting trace plots with appropriate size\n",
    "az.plot_trace(np.exp(idata.posterior), var_names=[\"intercept\", \"slopes\"], combined=True, figsize=(10, 6))\n",
    "az.plot_trace(idata.posterior, var_names=[\"alpha\"], combined=True, figsize=(10, 3))\n",
    "\n",
    "# Plotting posterior plots with appropriate size\n",
    "az.plot_posterior(np.exp(idata.posterior), var_names=[\"intercept\", \"slopes\"], figsize=(12, 3))\n",
    "az.plot_posterior(idata, var_names=[\"alpha\"], figsize=(4, 3))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting trace plots in logaritmic scale\n",
    "az.plot_trace(idata, figsize=(10, 9))\n",
    "az.plot_trace(idata, combined=True, figsize=(10, 9))\n",
    "az.plot_posterior(idata, figsize=(10, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    az.plot_ppc(idata, num_pp_samples=50, kind=\"scatter\")\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Summary Statistics**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transform coefficients to recover parameter values\n",
    "az.summary(np.exp(idata.posterior), var_names=[\"intercept\", \"slopes\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.summary(idata.posterior, var_names=[\"alpha\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CRPS evaluation**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Model with negative binomial distribution for likelihood (alpha Gamma distributed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify which model should be examined\n",
    "model = bayesian_model_nb_gamma\n",
    "idata = idata_bayesian_nb_gamma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Visualisations**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting trace with parameters transformed back to original scale\n",
    "# Plotting trace plots with appropriate size\n",
    "az.plot_trace(np.exp(idata.posterior), var_names=[\"intercept\", \"slopes\"], combined=True, figsize=(10, 6))\n",
    "az.plot_trace(idata.posterior, var_names=[\"alpha\"], combined=True, figsize=(10, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting posterior plots with appropriate size\n",
    "az.plot_posterior(np.exp(idata.posterior), var_names=[\"intercept\", \"slopes\"], figsize=(12, 3))\n",
    "az.plot_posterior(idata, var_names=[\"alpha\"], figsize=(4, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting trace plots in logaritmic scale\n",
    "az.plot_trace(idata, figsize=(10, 9))\n",
    "az.plot_trace(idata, combined=True, figsize=(10, 9))\n",
    "az.plot_posterior(idata, figsize=(10, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    az.plot_ppc(idata, num_pp_samples=50, kind=\"scatter\")\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Summary Statistics**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transform coefficients to recover parameter values\n",
    "az.summary(np.exp(idata.posterior), var_names=[\"intercept\", \"slopes\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.summary(idata.posterior, var_names=[\"alpha\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CRPS evaluation**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Model with Negative Binomial distribution (alpha Gamma distributed) and regression splines (B-splines)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify which model should be examined\n",
    "model = bayesian_model_nb_gamma_splines\n",
    "idata = idata_bayesian_nb_gamma_splines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Visualisations**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting trace with parameters transformed back to original scale\n",
    "# Plotting trace plots with appropriate size\n",
    "az.plot_trace(np.exp(idata.posterior), var_names=[\"intercept\", \"slopesX1\", \"slopesX2\"], combined=True, figsize=(10, 6))\n",
    "az.plot_trace(idata.posterior, var_names=[\"alpha\"], combined=True, figsize=(10, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting posterior plots with appropriate size\n",
    "az.plot_posterior(np.exp(idata.posterior), var_names=[\"intercept\", \"slopesX1\", \"slopesX2\"])\n",
    "az.plot_posterior(idata, var_names=[\"alpha\"], figsize=(4, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting trace plots in logaritmic scale\n",
    "az.plot_trace(idata, figsize=(10, 9))\n",
    "az.plot_trace(idata, combined=True, figsize=(10, 9))\n",
    "az.plot_posterior(idata)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    az.plot_ppc(idata, num_pp_samples=50, kind=\"scatter\")\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Summary Statistics**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transform coefficients to recover parameter values\n",
    "az.summary(np.exp(idata.posterior), var_names=[\"intercept\", \"slopesX1\", \"slopesX2\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.summary(idata.posterior, var_names=[\"alpha\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CRPS evaluation**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Model with Negative Binomial distribution (alpha Gamma distributed) and penalized regression splines via GRW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify which model should be examined\n",
    "model = bayesian_model_nb_gamma_penalized_splines_grw\n",
    "idata = idata_bayesian_nb_gamma_penalized_splines_grw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Summary Statistics**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transform coefficients to recover parameter values\n",
    "az.summary(np.exp(idata.posterior), var_names=[\"intercept\", \"slopesX1\", \"slopesX2\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.summary(idata.posterior, var_names=[\"alpha\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Visualisations**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting trace with parameters transformed back to original scale\n",
    "# Plotting trace plots with appropriate size\n",
    "az.plot_trace(np.exp(idata.posterior), var_names=[\"intercept\", \"slopesX1\", \"slopesX2\"], combined=True, figsize=(10, 6))\n",
    "az.plot_trace(idata.posterior, var_names=[\"alpha\"], combined=True, figsize=(10, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting posterior plots with appropriate size\n",
    "az.plot_posterior(np.exp(idata.posterior), var_names=[\"intercept\", \"slopesX1\", \"slopesX2\"])\n",
    "az.plot_posterior(idata, var_names=[\"alpha\"], figsize=(4, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_posterior(idata.posterior, var_names=[\"slopesX1\"], figsize=(25, 18))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting trace plots in logaritmic scale\n",
    "az.plot_trace(idata)\n",
    "az.plot_trace(idata, combined=True)\n",
    "az.plot_posterior(idata)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    az.plot_ppc(idata, num_pp_samples=50, kind=\"scatter\")\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6. Model with Negative Binomial distribution (alpha Gamma distributed) and penalized regression splines via pymc Potential"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify which model should be examined\n",
    "model = bayesian_model_nb_gamma_penalized_splines_penalty\n",
    "idata = idata_bayesian_nb_gamma_penalized_splines_penalty"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Summary Statistics**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transform coefficients to recover parameter values\n",
    "az.summary(np.exp(idata.posterior), var_names=[\"intercept\", \"slopesX1\", \"slopesX2\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.summary(idata.posterior, var_names=[\"alpha\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Visualisations**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting trace with parameters transformed back to original scale\n",
    "# Plotting trace plots with appropriate size\n",
    "az.plot_trace(np.exp(idata.posterior), var_names=[\"intercept\", \"slopesX1\", \"slopesX2\"], combined=True, figsize=(10, 6))\n",
    "az.plot_trace(idata.posterior, var_names=[\"alpha\"], combined=True, figsize=(10, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting posterior plots with appropriate size\n",
    "az.plot_posterior(np.exp(idata.posterior), var_names=[\"intercept\", \"slopesX1\", \"slopesX2\"])\n",
    "az.plot_posterior(idata, var_names=[\"alpha\"], figsize=(4, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_posterior(idata.posterior, var_names=[\"slopesX1\"], figsize=(25, 18))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting trace plots in logaritmic scale\n",
    "az.plot_trace(idata)\n",
    "az.plot_trace(idata, combined=True)\n",
    "az.plot_posterior(idata)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    az.plot_ppc(idata, num_pp_samples=50, kind=\"scatter\")\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CRPS evaluation**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(crps_average_gauss)\n",
    "print(crps_average_nb_half_normal)\n",
    "print(crps_average_nb_gamma)\n",
    "print(crps_average_nb_gamma_splines)\n",
    "print(crps_average_nb_gamma_penalized_splines_grw)\n",
    "print(crps_average_nb_gamma_penalized_splines_penalty)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## IV.2 PyStan Model Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Model with Negative Binomial distribution (alpha Gamma distributed) and penalized regression splines via GRW"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify which model should be examined\n",
    "model = stan_model_nb_gamma_penalized_splines_grw\n",
    "idata = idata_bayesian_nb_gamma_penalized_splines_grw_stan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Summary Statistics**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transform coefficients to recover parameter values\n",
    "az.summary(np.exp(idata.posterior))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.summary(idata.posterior)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Visualisations**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting trace with parameters transformed back to original scale\n",
    "# Plotting trace plots with appropriate size\n",
    "az.plot_trace(np.exp(idata.posterior), var_names=[\"intercept\", \"slopesX1\", \"slopesX2\"], combined=True, figsize=(10, 6))\n",
    "az.plot_trace(idata.posterior, var_names=[\"alpha\"], combined=True, figsize=(10, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting posterior plots with appropriate size\n",
    "az.plot_posterior(np.exp(idata.posterior), var_names=[\"intercept\", \"slopesX1\", \"slopesX2\"])\n",
    "az.plot_posterior(idata, var_names=[\"alpha\"], figsize=(4, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_posterior(idata.posterior, var_names=[\"slopesX1\"], figsize=(25, 18))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting trace plots in logaritmic scale\n",
    "az.plot_trace(idata)\n",
    "az.plot_trace(idata, combined=True)\n",
    "az.plot_posterior(idata)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    az.plot_ppc(idata, num_pp_samples=50, kind=\"scatter\")\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Model with Negative Binomial distribution (alpha Gamma distributed) and penalized regression splines via multivariate distribution using penalty matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify which model should be examined\n",
    "model = stan_model_nb_gamma_penalized_splines_multivariate\n",
    "idata = idata_bayesian_nb_gamma_penalized_splines_multivariate_stan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Summary Statistics**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Transform coefficients to recover parameter values\n",
    "az.summary(np.exp(idata.posterior))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.summary(idata.posterior)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Visualisations**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting trace with parameters transformed back to original scale\n",
    "# Plotting trace plots with appropriate size\n",
    "az.plot_trace(np.exp(idata.posterior), var_names=[\"intercept\", \"slopesX1\", \"slopesX2\"], combined=True, figsize=(10, 6))\n",
    "az.plot_trace(idata.posterior, var_names=[\"alpha\"], combined=True, figsize=(10, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting posterior plots with appropriate size\n",
    "az.plot_posterior(np.exp(idata.posterior), var_names=[\"intercept\", \"slopesX1\", \"slopesX2\"])\n",
    "az.plot_posterior(idata, var_names=[\"alpha\"], figsize=(4, 3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_posterior(idata.posterior, var_names=[\"slopesX1\"], figsize=(25, 18))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plotting trace plots in logaritmic scale\n",
    "az.plot_trace(idata)\n",
    "az.plot_trace(idata, combined=True)\n",
    "az.plot_posterior(idata)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    az.plot_ppc(idata, num_pp_samples=50, kind=\"scatter\")\n",
    "except Exception as error:\n",
    "    print(\"Error\", error)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## V. Test functionalities with simulated data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## VI. Data exploration steps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idata = idata_bayesian_nb_gamma_splines\n",
    "az.ess(idata, var_names=[\"intercept\", \"slopesX1\", \"slopesX2\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idata = idata_bayesian_nb_gamma_penalized_splines_penalty\n",
    "az.ess(idata, var_names=[\"intercept\", \"slopesX1\", \"slopesX2\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pm.math.exp(pm.math.exp(2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.exp(np.exp(2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_cm_features_allyears.loc[data_cm_features_allyears[\"country_id\"] == country_id, 'ged_sb']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.summary(idata)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "posterior_predictive = ppc_posterior_nb_gamma.get(\"posterior_predicitve\"); posterior_predictive"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ppc_posterior_nb_gamma"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "posterior_predictive[\"obs\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_bpv(idata, figsize=(12, 6))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "posterior_samples =  idata[\"posterior_predictive\"][\"obs\"].values\n",
    " # Reshape the array to a 2D array with samples in rows and observations in columns\n",
    "num_chains, num_draws, num_observations = posterior_samples.shape\n",
    "reshaped_samples = posterior_samples.reshape(num_chains * num_draws, num_observations)\n",
    "\n",
    "# Convert the 2D array to a pandas DataFrame\n",
    "df_posterior_samples = pd.DataFrame(reshaped_samples)\n",
    "\n",
    "# Optionally, you can add meaningful column names to the DataFrame\n",
    "# For example, assuming the original DataFrame has column names 'obs_0', 'obs_1', etc.\n",
    "column_names = ['obs_{}'.format(column_idx) for column_idx in range(num_observations)]\n",
    "df_posterior_samples.columns = column_names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test model with simulated data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##Step 1: Import Libraries\n",
    "import pymc as pm\n",
    "import pandas as pd\n",
    "import patsy as ps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import BSpline\n",
    "\n",
    "##Step 2: Generate simulated data\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 12345\n",
    "\n",
    "# Number of samples\n",
    "n_samples = 500\n",
    "\n",
    "# Generate X values\n",
    "X1 = np.sort(np.random.rand(n_samples) * 10)\n",
    "X2 = np.sort(np.random.rand(n_samples) * 10)\n",
    "\n",
    "# Number of knots\n",
    "num_knots = 10\n",
    "\n",
    "# Generate spline curves for X1 and X2\n",
    "knots1 = np.linspace(0, 10, num_knots)\n",
    "knots2 = np.linspace(0, 10, num_knots)\n",
    "\n",
    "coeffs1 = np.random.randn(num_knots)\n",
    "coeffs2 = np.random.randn(num_knots)\n",
    "\n",
    "spline1 = BSpline(knots1, coeffs1, k=3)\n",
    "spline2 = BSpline(knots2, coeffs2, k=3)\n",
    "\n",
    "f1_values = spline1(X1)\n",
    "f2_values = spline2(X2)\n",
    "\n",
    "# Generate Y values\n",
    "beta_0 = 0.5\n",
    "beta_1 = 1\n",
    "beta_2 = 1.2\n",
    "\n",
    "mu = np.exp(beta_0 + beta_1 * f1_values + beta_2 * f2_values)\n",
    "n = 2.0  # Adjust the size parameter as needed\n",
    "p = mu / (mu + n)  # Adjust the probability parameter as needed\n",
    "Y = np.random.negative_binomial(n, p, size=n_samples)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"ged_sb_tlag_1\": X1,\n",
    "    \"ged_sb_tsum_24\": X2,\n",
    "    \"ged_sb\": Y\n",
    "})\n",
    "\n",
    "## Step 3: Visualize the Simulated Data\n",
    "# Visualizing data\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot X1 vs. Y\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(df['ged_sb_tlag_1'], df['ged_sb'], color='blue', s=10, alpha=0.5)\n",
    "plt.plot(X1, mu, color='red', linewidth=2)\n",
    "plt.title('X1 vs. Y')\n",
    "plt.xlabel('ged_sb_tlag_1')\n",
    "plt.ylabel('ged_sb')\n",
    "\n",
    "# Plot X2 vs. Y\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(df['ged_sb_tsum_24'], df['ged_sb'], color='green', s=10, alpha=0.5)\n",
    "plt.plot(X2, mu, color='red', linewidth=2)\n",
    "plt.title('X2 vs. Y')\n",
    "plt.xlabel('ged_sb_tsum_24')\n",
    "plt.ylabel('ged_sb')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##Step 4: Define the PyMc Model\n",
    "dataX1 = df.ged_sb_tlag_1.to_numpy()\n",
    "dataX2 = df.ged_sb_tsum_24.to_numpy()\n",
    "dataY = df.ged_sb.to_numpy()\n",
    "\n",
    "# Note: If knots are intended to be not equally spaces (e.g. set by quantiles) the consequence on penalized splines has to be evaluated\n",
    "# Define the knot positions\n",
    "\n",
    "\n",
    "# We define the knot list such that we'll have num_knots knot inbetween the boundaries of the data\n",
    "knot_list_X1 = np.linspace(dataX1.min(), dataX1.max(), num_knots+2)[1:-1]\n",
    "knot_list_X2 = np.linspace(dataX2.min(), dataX2.max(), num_knots+2)[1:-1]\n",
    "\n",
    "# Define the basis functions\n",
    "# Note: We include the intercept once in the model via the intercept parameter, to prevent having additional '1' columns in basis functions include_intercept is set to False and automatic creation of '1' column of patsy is prevented by '-1'\n",
    "basis_X1 = ps.dmatrix(\"bs(X1, knots=knots, degree=3, include_intercept=False) -1\", {\"X1\": dataX1, \"knots\": knot_list_X1})\n",
    "basis_X2 = ps.dmatrix(\"bs(X2, knots=knots, degree=3, include_intercept=False) -1\", {\"X2\": dataX2, \"knots\": knot_list_X2})\n",
    "\n",
    "# Define variables for tau priors\n",
    "a_X1 = a_X2 = 1\n",
    "b_X1 = b_X2 = 0.005\n",
    "\n",
    "def generate_penalty_matrix(no_coefficients):\n",
    "    penalty_matrix = -2 * np.eye(no_coefficients) + np.eye(no_coefficients, k=1) + np.eye(no_coefficients, k=-1)\n",
    "    penalty_matrix[0, 0] = 1\n",
    "    penalty_matrix[no_coefficients - 1, no_coefficients - 1] = 1\n",
    "    return penalty_matrix\n",
    "\n",
    "\n",
    "no_X1_coefficients = basis_X1.shape[1]\n",
    "no_X2_coefficients = basis_X2.shape[1]\n",
    "K_X1 = generate_penalty_matrix(no_X1_coefficients)\n",
    "K_X2 = generate_penalty_matrix(no_X2_coefficients)\n",
    "\n",
    "COORDS_penalized_splines = \\\n",
    "    {\"obs_idx\": df.index, \"splines_x1_rest\": np.arange(basis_X1.shape[1] -1), \"splines_x2_rest\": np.arange(basis_X2.shape[1] -1)}\n",
    "\n",
    "# NOTE: WIP: Implementation of penalized regression splines still to be figured out, Version with Penalty deduction in likelihood\n",
    "with pm.Model(coords=COORDS_penalized_splines) as bayesian_model_nb_gamma_penalized_splines_penalty:\n",
    "    # Specify the data\n",
    "    X1 = pm.ConstantData(\"fatality lag t=1\", dataX1, dims=\"obs_idx\")\n",
    "    X2 = pm.ConstantData(\"fatality rolling average t=24\", dataX2, dims=\"obs_idx\")\n",
    "    Y = pm.ConstantData(\"fatility count\", dataY, dims=\"obs_idx\")\n",
    "\n",
    "    # Define the priors for regressors and negative binomial over-dispersion parameter\n",
    "    intercept = pm.Normal(\"intercept\", sigma=10)\n",
    "\n",
    "    # Define first order random walk to incorporate regularization\n",
    "    tau_X1 = pm.InverseGamma(\"tauX1\", alpha=a_X1, beta=b_X1)\n",
    "    tau_X2 = pm.InverseGamma(\"tauX2\", alpha=a_X2, beta=b_X2)\n",
    "\n",
    "    # Create deterministic variables for squared taus\n",
    "    tau_X1_squared = pm.Deterministic(\"tau_X1_squared\", tau_X1 ** 2)\n",
    "    tau_X2_squared = pm.Deterministic(\"tau_X2_squared\", tau_X2 ** 2)\n",
    "\n",
    "    # Introduce flat priors for the first regression coeffcients of the splines\n",
    "    beta_X1_0 = pm.Normal(0, 1000, \"slopesX1_0\")\n",
    "    beta_X2_0 = pm.Normal(0, 1000, \"slopesX2_0\")\n",
    "    \n",
    "    # Define remaining betas as normal variables with standard deviation tau\n",
    "    beta_X1_rest = pm.Normal(\"slopesX1_rest\", 0.0, tau_X1, dims=\"splines_x1_rest\")\n",
    "    beta_X2_rest = pm.Normal(\"slopesX2_rest\", 0.0, tau_X2, dims=\"splines_x2_rest\")\n",
    "    \n",
    "    # Concatenate the flat priors to the beginning of the coefficients\n",
    "    beta_X1 = pm.Deterministic(\"slopesX1\", pm.math.concatenate([[beta_X1_0], beta_X1_rest]))\n",
    "    beta_X2 = pm.Deterministic(\"slopesX2\", pm.math.concatenate([[beta_X2_0], beta_X2_rest]))\n",
    "\n",
    "\n",
    "    alpha = pm.Gamma('alpha', 0.1, 0.1)\n",
    "\n",
    "    # Define mean of negative binomial distribution\n",
    "    mu = pm.math.exp(intercept + pm.math.dot(np.asarray(basis_X1, order=\"F\"), beta_X1.T) + pm.math.dot(np.asarray(basis_X2, order=\"F\"), beta_X2.T))\n",
    "\n",
    "    # Define the likelihood\n",
    "    obs = pm.NegativeBinomial(\"obs\", mu=mu, alpha=alpha, observed=Y, dims=\"obs_idx\")\n",
    "\n",
    "    # Define the penalty term and incorporate it using pm.Potential\n",
    "    penalty_term_X1 = pm.math.dot(beta_X1.T, pm.math.dot(K_X1, beta_X1)) / (2 * tau_X1_squared)\n",
    "    penalty_X1 = pm.Potential(\"penalty_X1\", -penalty_term_X1)\n",
    "\n",
    "    penalty_term_X2 = pm.math.dot(beta_X2, pm.math.dot(K_X2, beta_X2)) / (2 * tau_X2_squared)\n",
    "    penalty_X2 = pm.Potential(\"penalty_X2\", -penalty_term_X2)\n",
    "\n",
    "    # Run the sampling using the No-U-Turn Sampler (NUTS) for the specified number of samples\n",
    "    idata_bayesian_nb_gamma_penalized_splines_penalty = pm.sample(draws=draws_default, tune=tune_default, random_seed=RANDOM_SEED, target_accept=target_accept_default)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Code which is WIP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Note: WIP\n",
    "def model_training(df:pd.DataFrame, model_params: dict, validation_year: int, K:int):\n",
    "    # Extracting year and month from month_id\n",
    "    df['year'] = ((df['month_id'] - 1) // 12) + 1990\n",
    "    df['month'] = ((df['month_id'] - 1) % 12) + 1\n",
    "\n",
    "    # Step 1: Split data into training and test sets\n",
    "    train_data = df[(df['year'] >= 1990) & (df['year'] < 2018)]\n",
    "    test_data = df[df['year'] == 2018]\n",
    "\n",
    "    # Step 2: Perform \"10-Fold\" time series cross-validation within the training set with a 2-month-gap\n",
    "    tscv = TimeSeriesSplit(n_splits=K)\n",
    "\n",
    "    for train_index, val_index in tscv.split(train_data):\n",
    "        # Extract train and validation sets for the current fold\n",
    "        train_fold = train_data.iloc[train_index]\n",
    "        val_fold = train_data.iloc[val_index]\n",
    "\n",
    "        # Exclude last two months (November and December) from the training set\n",
    "        train_fold = train_fold[~((train_fold['month'] == 11) | (train_fold['month'] == 12))]\n",
    "\n",
    "        # Step 3: Train the model on the training set\n",
    "        model, trace = build_bayesian_model(df, model_params)\n",
    "\n",
    "        # Step 4: Predict the validation set\n",
    "        crps_scores, crps_average = calculate_crps(trace, val_fold)\n",
    "\n",
    "\n",
    "# Note WIP\n",
    "model_params = {\n",
    "    \"coords\": COORDS,\n",
    "    \"intercept_prior_sigma\": 0.1,\n",
    "    \"slopes_prior_mu\": 0.0,\n",
    "    \"slopes_prior_sd\": 10,\n",
    "    \"alpha_prior_alpha\": 0.1,\n",
    "    \"alpha_prior_beta\": 0.1,\n",
    "    \"regressors_data\": [df.ged_sb_tlag_1.to_numpy(), df.ged_sb_tsum_24.to_numpy()],  # Add more predictors' data as needed\n",
    "    \"Y_data\": df.ged_sb.to_numpy(),\n",
    "    \"regularization_param\": 0.01,  # Add your desired regularization parameter value\n",
    "    \"num_knots\": 10,  # Add your desired number of knots for P-splines\n",
    "    \"draws\": 1000,\n",
    "    \"tune\": 2000,\n",
    "    \"random_seed\": rng,\n",
    "    \"target_accept\": 0.95\n",
    "}\n",
    "\n",
    "# Note: WIP\n",
    "def build_bayesian_model(df: pd.DataFrame(), model_params: dict):\n",
    "    \"\"\"\n",
    "    Build the Bayesian model for the negative binomial regression with P-splines.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing the data.\n",
    "        model_params (dict): Dictionary containing the model parameters.\n",
    "\n",
    "    Returns:\n",
    "        model: PyMC3 model object\n",
    "        trace: PyMC3 trace object\n",
    "    \"\"\"\n",
    "    # Extract the model parameters\n",
    "    coords = model_params[\"coords\"]\n",
    "    intercept_prior_sigma = model_params[\"intercept_prior_sigma\"]\n",
    "    slopes_prior_mu = model_params[\"slopes_prior_mu\"]\n",
    "    slopes_prior_sd = model_params[\"slopes_prior_sd\"]\n",
    "    alpha_prior_alpha = model_params[\"alpha_prior_alpha\"]\n",
    "    alpha_prior_beta = model_params[\"alpha_prior_beta\"]\n",
    "    regressors_data = model_params[\"regressors_data\"]\n",
    "    Y_data = model_params[\"Y_data\"]\n",
    "    regularization_param = model_params[\"regularization_param\"]\n",
    "    num_knots = model_params[\"num_knots\"]\n",
    "    draws = model_params[\"draws\"]\n",
    "    tune = model_params[\"tune\"]\n",
    "    random_seed = model_params[\"random_seed\"]\n",
    "    target_accept = model_params[\"target_accept\"]\n",
    "\n",
    "    # Build the model\n",
    "    with pm.Model(coords=coords) as bayesian_model_nb:\n",
    "        # Define the priors for the intercept, slopes, and over-dispersion parameter\n",
    "        intercept = pm.Normal(\"intercept\", 0.0, intercept_prior_sigma)\n",
    "        slopes = pm.Normal(\"slopes\", slopes_prior_mu, slopes_prior_sd, dims=\"predictors\")\n",
    "        alpha = pm.Gamma('alpha', alpha_prior_alpha, alpha_prior_beta)\n",
    "\n",
    "        # Specify the data\n",
    "        X = []\n",
    "        for i in range(len(regressors_data)):\n",
    "            X.append(pm.ConstantData(\"X{}\".format(i), regressors_data[i], dims=\"obs_idx\"))\n",
    "        Y = pm.ConstantData(\"Y\", Y_data, dims=\"obs_idx\")\n",
    "\n",
    "        # Define the basis functions\n",
    "        knots = np.linspace(df['month_id'].min(), df['month_id'].max(), num_knots + 2)[1:-1]\n",
    "        basis_funcs = pm.spline.make_spline_basis(knots, degree=3)\n",
    "\n",
    "        # Define the coefficients for the basis functions\n",
    "        beta = pm.Normal(\"beta\", mu=0, sigma=regularization_param, dims=\"basis_funcs\")\n",
    "\n",
    "        # Define the mean of the negative binomial distribution\n",
    "        mu = pm.math.exp(intercept + pm.math.dot(X, slopes) + pm.math.dot(basis_funcs, beta))\n",
    "\n",
    "        # Define the likelihood\n",
    "        obs = pm.NegativeBinomial(\"obs\", mu=mu, alpha=alpha, observed=Y, dims=\"obs_idx\")\n",
    "\n",
    "        # Run the sampling using the No-U-Turn Sampler (NUTS) for the specified number of samples\n",
    "        trace = pm.sample(draws=draws, tune=tune, random_seed=random_seed, target_accept=target_accept)\n",
    "\n",
    "        return bayesian_model_nb, trace\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
