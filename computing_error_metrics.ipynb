{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import CRPS.CRPS as crps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data_gathering import gather_data_actuals"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca65f59682bcc069"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Notebook to compute the three error metrics CRPS, Interval Score and Ignorance Score and export the results in form of a dictionary of dataframes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa8fb4d164330538"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**The code for computing the metrics is based on the code provided by the VIEWS team for this purpose in the context of the VIEWS prediction.**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20da2d5877e1064"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Function to fill posterior predictive samples for countries not part of the training with zero observed values**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bce8f290e49e8961"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fill_pps_for_zero_obs_countries(df: pd.DataFrame, actual_countries: list, actuals_data: pd.DataFrame):\n",
    "    # Unique country_ids in the original DataFrame\n",
    "    unique_country_ids = df['country_id'].unique()\n",
    "    # Determine missing country_ids\n",
    "    missing_country_ids = [country_id for country_id in actual_countries if country_id not in unique_country_ids]\n",
    "    # Unique month_ids in the original DataFrame\n",
    "    unique_month_ids = df['month_id'].unique()\n",
    "    # Create an empty list to store new rows\n",
    "    new_rows = []\n",
    "    \n",
    "    # Generate new rows\n",
    "    for month_id in unique_month_ids:\n",
    "        for country_id in missing_country_ids:\n",
    "            observation = actuals_data.loc[(actuals_data['country_id'] == country_id) & (actuals_data['month_id'] == month_id), 'ged_sb'].values\n",
    "            new_row = {'month_id': month_id, 'country_id': country_id, 'ged_sb': observation[0]}\n",
    "            for sample_col in [f'draw_{i}' for i in range(1, 100)]:\n",
    "                new_row[sample_col] = 0.0\n",
    "            new_rows.append(new_row)\n",
    "    \n",
    "    # Create a new DataFrame from the list of new rows\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    \n",
    "    # Append the new DataFrame to the original DataFrame\n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    # Optionally, you can sort the DataFrame\n",
    "    df = df.sort_values(by=['country_id', 'month_id']).reset_index(drop=True)\n",
    "    \n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7287648262a35547"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Data and Setting the Stage for the Computations"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d43f5d6333e0bb7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load 'Actuals' data\n",
    "_, _, _, _, data_cm_actual_allyears \\\n",
    "    = gather_data_actuals()\n",
    "# Determine all countries\n",
    "actual_countries = data_cm_actual_allyears['country_id'].unique()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9db830f8d91e2569"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load posterior predictive samples from Bayesian models\n",
    "model_original_identifier = ['baseline_f_m', 'baseline_f', 'model13_nb_feature_set1', 'model3_zinb_feature_set1', 'model1_zinb_feature_set1', 'model4_zinb_feature_set1', 'model15_zinb_feature_set1', 'model19_zinb_feature_set3', 'model23_zinb_feature_set4']\n",
    "data_path = 'C:/Users/Uwe Drauz/Documents/bachelor_thesis_local/personal_competition_data/Results/'\n",
    "pps_list = [pd.read_parquet(fr'C:\\Users\\Uwe Drauz\\Documents\\bachelor_thesis_local\\personal_competition_data\\Results\\{model_identifier}_posterior_predicitve_samples_wt.parquet') for model_identifier in model_original_identifier]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "445bdd358fad9104"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply the function 'fill_pps_for_zero_obs_countries' to each DataFrame in the list\n",
    "for i, pps in enumerate(pps_list):\n",
    "    pps_list[i] = fill_pps_for_zero_obs_countries(pps, actual_countries, data_cm_actual_allyears)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a6c11bf742a25cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "export_path = r'C:\\Users\\Uwe Drauz\\Documents\\bachelor_thesis_local\\personal_competition_data\\Results\\\\'\n",
    "model_identifiers = ['baseline_f_m', 'baseline_f', 'model1', 'model2', 'model3', 'model4', 'model5', 'model6', 'model7']\n",
    "score_types = ['crps', 'interval_score', 'ignorance_score']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42a538831ec294dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Functions to compute the three error metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efb972c779176d53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define bins for the Ignorance Score\n",
    "bins = [\n",
    "        0,\n",
    "        0.5,\n",
    "        2.5,\n",
    "        5.5,\n",
    "        10.5,\n",
    "        25.5,\n",
    "        50.5,\n",
    "        100.5,\n",
    "        250.5,\n",
    "        500.5,\n",
    "        1000.5,\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4190b4a9f1b334b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Ignorance Score**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "983aaf113d85bcc6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# \n",
    "def _ensemble_ignorance_score(predictions, n, observed):\n",
    "        c = Counter(predictions)\n",
    "        # n = c.total() : this works from python version 3.10, avoid this for a while.\n",
    "        prob = c[observed] / n # if counter[observed] is 0, then this returns correctly\n",
    "        return -np.log2(prob)\n",
    "\n",
    "def ensemble_ignorance_score(observations, forecasts, bins, low_bin = 0, high_bin = 10000):\n",
    "    \"\"\"\n",
    "    This implements the Ensemble (Ranked) interval Score from the easyVerification R-package in Python. Also inspired by properscoring.crps_ensemble(),\n",
    "    and has interface that works with the xskillscore package.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observations : float or array_like\n",
    "        Observations float or array. Missing values (NaN) are given scores of\n",
    "        NaN.\n",
    "    forecasts : float or array_like\n",
    "        Array of forecasts ensemble members, of the same shape as observations\n",
    "        except for the axis along which RIGN is calculated (which should be the\n",
    "        axis corresponding to the ensemble). If forecasts has the same shape as\n",
    "        observations, the forecasts are treated as deterministic. Missing\n",
    "        values (NaN) are ignored.\n",
    "    round_values: converts input data to integers by rounding.\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out : np.ndarray\n",
    "        RIGN for each ensemble forecast against the observations.\n",
    "    \"\"\"\n",
    "    observations = np.asarray(observations)\n",
    "    forecasts = np.asarray(forecasts)\n",
    "\n",
    "    assert np.all((forecasts >= 0) | np.isnan(forecasts)), \"Forecasts must be non-negative or NaN.\"\n",
    "    assert np.all(observations >= 0), f\"Observations must be non-negative.\"\n",
    "\n",
    "    assert isinstance(bins, (int, list)), f\"bins must be an integer or a list with floats\"\n",
    "    if isinstance(bins, int):\n",
    "        assert bins > 0, f\"bins must be an integer above 0 or a list with floats.\"\n",
    "\n",
    "    def digitize_minus_one(x, bins, right=False):\n",
    "        return np.digitize(x, bins, right) - 1\n",
    "\n",
    "    \"\"\" edges = np.histogram_bin_edges(forecasts[..., :], bins = bins, range = (low_bin, high_bin))\n",
    "    binned_forecasts =  np.apply_along_axis(digitize_minus_one, axis = 1, arr = forecasts, bins = edges)\n",
    "    binned_observations = digitize_minus_one(observations, edges) \"\"\"\n",
    "\n",
    "    edges = np.histogram_bin_edges(forecasts, bins=bins, range=(low_bin, high_bin))\n",
    "    binned_forecasts = digitize_minus_one(forecasts, edges)\n",
    "    binned_observations = digitize_minus_one(observations, edges)\n",
    "\n",
    "\n",
    "    # Append one observation in each bin-category to the forecasts to prevent 0 probability occuring.\n",
    "    unique_categories = np.arange(0, len(bins))\n",
    "    binned_forecasts = np.concatenate((binned_forecasts, np.tile(unique_categories, binned_forecasts.shape[:-1] + (1,))), axis = -1)\n",
    "    \n",
    "    n = binned_forecasts.shape[-1]\n",
    "\n",
    "    #if observations.shape == forecasts.shape:\n",
    "        # exact prediction yields 0 ign\n",
    "    ign_score = np.empty_like(binned_observations, dtype = float)\n",
    "    for index in np.ndindex(ign_score.shape):\n",
    "        ign_score[index] = _ensemble_ignorance_score(binned_forecasts[index], n, binned_observations[index])\n",
    "    \n",
    "    \n",
    "    return ign_score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56fec647cec89626"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Interval Score**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbea0cf5f9de2c02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def interval_score(observed: np.array, predictions: np.array, prediction_interval_level: float = 0.90) -> np.array:\n",
    "    \"\"\"\n",
    "    Interval Score implemented based on the scaled Mean Interval Score in the R tsRNN package https://rdrr.io/github/thfuchs/tsRNN/src/R/metrics_dist.R\n",
    "\n",
    "    The Interval Score is a probabilistic prediction evaluation metric that weights between the narrowness of the forecast range and the ability to correctly hit the observed value within that interval.\n",
    "    \n",
    "    :param observed: observed values\n",
    "    :type observed: array_like\n",
    "    :param predictions: probabilistic predictions with the latter axis (-1) being the forecasts for each observed value\n",
    "    :type predictions: array_like\n",
    "    :param prediction_interval_level: prediction interval between [0, 1]\n",
    "    :type prediction_interval_level: float\n",
    "    :returns array_like with the interval score for each observed value\n",
    "    :rtype array_like\n",
    "\n",
    "    observed = np.random.negative_binomial(5, 0.8, size = 600)\n",
    "    forecasts = np.random.negative_binomial(5, 0.8, size = (600, 1000))\n",
    "\n",
    "    score = interval_score(observed, forecasts)\n",
    "    print(f'MIS: {score.mean()}')\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    assert 0 < prediction_interval_level < 1, f\"'prediction_interval_level' must be a number between 0 and 1.\" \n",
    "\n",
    "    alpha = 1 - prediction_interval_level\n",
    "    lower = np.quantile(predictions, q = alpha/2, axis = -1)\n",
    "    upper = np.quantile(predictions, q = 1 - (alpha/2), axis = -1)\n",
    "\n",
    "    interval_width = upper - lower\n",
    "    lower_coverage = (2/alpha)*(lower-observed) * (observed<lower)\n",
    "    upper_coverage = (2/alpha)*(observed-upper) * (observed>upper)\n",
    "\n",
    "    return (interval_width + lower_coverage + upper_coverage)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Computing metics (including CRPS)**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccebf160ec79fe18"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_metrics(pps: pd.DataFrame, export_path: str, model_identifier: str, export = True):\n",
    "    \"\"\"\n",
    "    Compute CRPS, Interval Score and Ignorance Score for each country_id and month_id based on posterior predicitve samples\n",
    "    Args:\n",
    "        pps: \n",
    "        export_path: \n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    # Extract column names that start with 'draw_' from pps\n",
    "    predictions_column_names = [col for col in pps.columns if col.startswith('draw_')]\n",
    "    for country_id in pps['country_id'].unique():\n",
    "        for month_id in pps['month_id'].unique():\n",
    "            # 1. Take value in 'ged_sb' as observed value, take values in 'draw_1', 'draw_2', ... as predictions for interval score and forecasts for ignorance score\n",
    "            observed = pps.loc[(pps['country_id'] == country_id) & (pps['month_id'] == month_id), 'ged_sb'].values\n",
    "            predictions = pps.loc[(pps['country_id'] == country_id) & (pps['month_id'] == month_id), predictions_column_names].values.flatten()\n",
    "            predictions = predictions[~np.isnan(predictions)].reshape(1, -1)\n",
    "            # 2. Calculate interval score and ignorance score for each country_id and month_id and save in dataframe\n",
    "            interval_score_value = interval_score(observed, predictions)[0]\n",
    "            ignorance_score_value = ensemble_ignorance_score(observed, predictions, bins = bins)[0]\n",
    "            crps_score, _, _ = crps(predictions.flatten(), observed).compute()\n",
    "            # 3. Save in dataframe\n",
    "            # Append the result as a dictionary to the results list\n",
    "            results_list.append({\n",
    "                'country_id': country_id, \n",
    "                'month_id': month_id, \n",
    "                'crps': crps_score, \n",
    "                'interval_score': interval_score_value, \n",
    "                'ignorance_score': ignorance_score_value\n",
    "            })\n",
    "    \n",
    "    # Convert the list of dictionaries to a dataframe\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    results_df['crps'] = results_df['crps'].astype('float64')\n",
    "    \n",
    "    if export:\n",
    "        results_df.to_parquet(export_path + f'{model_identifier}_metrics_by_country_and_month.parquet')\n",
    "    return results_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67c3feadf514e8f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Performing different groupings of the data**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4a0102d02493e03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_averages(df, score_type):\n",
    "    start_month_id = 457\n",
    "    year_length = 12\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Year Average']\n",
    "    years = [2018, 2019, 2020, 2021]\n",
    "\n",
    "    results_df = pd.DataFrame(columns=['Year'] + months)\n",
    "    results_df['Year'] = years\n",
    "\n",
    "    for year in years:\n",
    "        yearly_scores = []\n",
    "        for month_index in range(1, year_length + 1):\n",
    "            month_id = start_month_id + (year - years[0]) * year_length + (month_index - 1)\n",
    "            mean_score = df.loc[df['month_id'] == month_id, score_type].mean()\n",
    "            yearly_scores.append(mean_score)\n",
    "        yearly_scores.append(pd.Series(yearly_scores).mean())\n",
    "        results_df.loc[results_df['Year'] == year, 1:] = yearly_scores\n",
    "\n",
    "    return results_df\n",
    "\n",
    "def compute_averages_by_country(df, score_type):\n",
    "    start_month_id = 457\n",
    "    year_length = 12\n",
    "    months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Year Average']\n",
    "    years = [2018, 2019, 2020, 2021]\n",
    "    country_ids = df['country_id'].unique()\n",
    "\n",
    "    all_countries_df = pd.DataFrame()\n",
    "\n",
    "    for country_id in country_ids:\n",
    "        country_df = pd.DataFrame(columns=['Year', 'Country_ID'] + months)\n",
    "        country_df['Year'] = years\n",
    "        country_df['Country_ID'] = country_id\n",
    "\n",
    "        for year in years:\n",
    "            yearly_scores = []\n",
    "            for month_index in range(1, year_length + 1):\n",
    "                month_id = start_month_id + (year - years[0]) * year_length + (month_index - 1)\n",
    "                mean_score = df[(df['month_id'] == month_id) & (df['country_id'] == country_id)][score_type].mean()\n",
    "                yearly_scores.append(mean_score)\n",
    "            yearly_scores.append(pd.Series(yearly_scores).mean())\n",
    "            country_df.loc[country_df['Year'] == year, 2:] = yearly_scores\n",
    "\n",
    "        all_countries_df = all_countries_df.append(country_df, ignore_index=True)\n",
    "\n",
    "    return all_countries_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6021ffe75e569ab0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computing the metrics and exporting the results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba78e50fbfd9ed1c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**As a dictionary including grouping the data**\n",
    "(This version is used for:)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e5552c95806e718"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute the metrics and store them in a dictionary of dataframes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fabaae8d812f1f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_dict = {}\n",
    "for model_identifier, pps in zip(model_identifiers, pps_list):\n",
    "    # Compute metrics for each model\n",
    "    results_df = compute_metrics(pps, export_path, model_identifier, export = False)\n",
    "    \n",
    "    # Compute averages for each score type\n",
    "    # Dictionaries to store the computed dataframes\n",
    "    averages_overall_dict = {}\n",
    "    averages_dict = {}\n",
    "    averages_by_country_dict = {}\n",
    "    \n",
    "    for score_type in score_types:\n",
    "        # Compute averages for each score type\n",
    "        averages_dict[score_type] = compute_averages(results_df, score_type)\n",
    "        \n",
    "        # Compute averages by country for each score type\n",
    "        averages_by_country_dict[score_type] = compute_averages_by_country(results_df, score_type)\n",
    "        \n",
    "        # Compute overall averages for each score type\n",
    "        averages_overall_dict[score_type] = averages_dict[score_type]['Year Average'].mean()\n",
    "        \n",
    "        # Store average dictionaries in scores_dict as nested dictionary with model identifier as key\n",
    "        scores_dict[model_identifier] = {\n",
    "            'average_over_all_months': averages_overall_dict,\n",
    "            'averages_per_month_and_year': averages_dict,\n",
    "            'averages_per_montg_year_and_country': averages_by_country_dict\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8c44b0656f0a594"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Export the dataframes in the scores_dict to parquet files with suitable names"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec45d75fe6caeda2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export the dataframes in the scores_dict to parquet files with suitable names\n",
    "for model_identifier, model_dict in scores_dict.items():\n",
    "    for score_type, score_dict in model_dict.items():\n",
    "        for score_name, score_df in score_dict.items():\n",
    "            # If score_df is numpy.float64, convert it to a dataframe with one row and one column\n",
    "            if isinstance(score_df, np.float64):\n",
    "                score_df = pd.DataFrame(score_df, index=[0], columns=[score_name])\n",
    "            score_df.to_parquet(export_path + f'metrics/{model_identifier}_{score_type}_{score_name}.parquet')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e9245f789f91068"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**As a dataframe (without grouping the data and with fatalities)**\n",
    "(This version is used for:)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "228e6a924d072b77"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute the metrics without grouping the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ac4b2eda2328a12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute metrics for each model and concatenate the results in one dataframe\n",
    "results_df = pd.DataFrame()\n",
    "for model_identifier, pps in zip(model_identifiers, pps_list):\n",
    "    # Compute metrics for each model\n",
    "    metrics_model = compute_metrics(pps, export_path, model_identifier, export = False)\n",
    "    # Append the results to the results_df together with the model identifier\n",
    "    results_df = results_df.append(metrics_model.assign(model_identifier=model_identifier), ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68128c248f45fc21"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Add the actual fatalities to the results_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "326d13572cb15483"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Join the column 'ged_sb' from data_cm_actual_allyears to the results_df based on the columns 'country_id' and 'month_id'\n",
    "results_df = results_df.merge(data_cm_actual_allyears[['country_id', 'month_id', 'ged_sb']], on=['country_id', 'month_id'], how='left')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1826cc021f1b5df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Export the results to a parquet file and a csv file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd9e4e219442574b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export the concatenated dataframe to a parquet file as mertrics_structured.parquet\n",
    "results_df.to_parquet(export_path + 'metrics/metrics_structured.parquet')\n",
    "results_df.to_csv(export_path + 'metrics/metrics_structured.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af53d4be969cfdb8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the results from the parquet file\n",
    "results_df = pd.read_parquet(export_path + 'metrics/metrics_structured.parquet')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ab64240d880f963"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# There was an error in the previous implementation. I need to add the 'year' column to the original dataframe\n",
    "# before grouping by 'country_id', 'year', and 'model_identifier'.\n",
    "\n",
    "def process_data_corrected(df):\n",
    "    # Mapping month_id to years\n",
    "    month_to_year = {month: year for year in range(2018, 2022) for month in range(457 + 12 * (year - 2018), 457 + 12 * (year - 2018) + 12)}\n",
    "\n",
    "    # Apply the mapping to the month_id column to create a year column\n",
    "    df['year'] = df['month_id'].map(month_to_year)\n",
    "\n",
    "    # Group by country_id, year, and model identifier\n",
    "    grouped_df = df.groupby(['country_id', 'year', 'model_identifier']).agg({\n",
    "        'crps': 'mean',\n",
    "        'interval_score': 'mean',\n",
    "        'ignorance_score': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    return grouped_df\n",
    "\n",
    "# Process the data with the corrected function\n",
    "processed_data_corrected = process_data_corrected(results_df)\n",
    "\n",
    "# Display the first few rows of the processed data\n",
    "processed_data_corrected.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44dd2b842402a080"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_data_corrected"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f2756f31423005e4"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d972b3927c5a4224"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
